{
  "search_metadata": {
    "total_jobs": 20,
    "search_date": "1752510342.274726",
    "roles_searched": [
      "Machine Learning Engineer",
      "Machine Learning Scientist",
      "ML Engineer",
      "ML Scientist",
      "Artificial Intelligence Engineer",
      "AI Engineer",
      "AI Scientist",
      "Data Scientist",
      "Applied Scientist",
      "Research Scientist",
      "ML Research Engineer",
      "AI Research Engineer",
      "Deep Learning Engineer",
      "Computer Vision Engineer",
      "NLP Engineer",
      "Natural Language Processing Engineer",
      "Computer Vision Scientist",
      "NLP Scientist",
      "Deep Learning Scientist",
      "MLOps Engineer",
      "ML Platform Engineer",
      "AI Platform Engineer",
      "Senior Machine Learning Engineer",
      "Senior ML Engineer",
      "Senior AI Engineer",
      "Lead Machine Learning Engineer",
      "Lead ML Engineer",
      "Lead AI Engineer",
      "Principal Machine Learning Engineer",
      "Principal ML Engineer",
      "Principal AI Engineer",
      "Machine Learning Developer",
      "AI Developer",
      "ML Developer",
      "Machine Learning Specialist",
      "AI Specialist",
      "ML Specialist",
      "Machine Learning Architect",
      "AI Architect",
      "ML Architect",
      "Machine Learning Researcher",
      "AI Researcher",
      "ML Researcher",
      "Research Engineer",
      "Applied Research Scientist",
      "Machine Learning Research Engineer",
      "AI Research Scientist",
      "ML Software Engineer",
      "AI Software Engineer",
      "Machine Learning Software Engineer",
      "AI/ML Engineer",
      "ML/AI Engineer",
      "Machine Learning & AI Engineer",
      "AI & Machine Learning Engineer"
    ],
    "search_location": "Global",
    "jobs_per_role": 30,
    "max_total_jobs": 20,
    "seconds_back": 3600,
    "days_back": 0,
    "detailed_job_info": true,
    "job_skills_info": true
  },
  "jobs": [
    {
      "id": "4264482018",
      "title": "Software Unit & Integration Test Engineer",
      "company": "Tata Technologies",
      "location": "Bengaluru, Karnataka, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264482018",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264482018",
      "company_url": "",
      "description": "üì¢ We‚Äôre Hiring! | Software Unit & Integration Test Engineerüìç Location: BangaloreüìÖ Experience: 5+ Yearsüïí Joining Timeline: Immediate to 30 Days\nüîß Key Responsibilities:Perform Software Unit (SWE.4) & Integration Testing (SWE.5) as per ASPICE processesDevelop, execute & maintain test cases/scriptsDebug issues using tools like CANoe, CANape, Trace32, ECUTestAutomate testing with Python & CAPL scriptingUse Vector VFlash for flashing & diagnosticsCollaborate with development teams and prepare quality reports\n‚úÖ Must-Have Skills:CANoe, CANape, VFlash, DaVinci Configurator/DeveloperECUTest, Trace32, ETAS INCA, ZenzefiTasking Compiler, CMake, MinGWPython & CAPL scriptingStrong understanding of ECU, CAN, UDS protocols\nüì© Interested?Share your profile with Shital.jakkulwar@tatatechnologies.com along with:Total ExperienceRelevant ExperienceCurrent & Preferred LocationCurrent CTC & Expected CTCNotice Period",
      "skills": [
        "Unit Testing",
        "Advanced Driver-Assistance Systems (ADAS)",
        "CANape",
        "Integration Testing",
        "Test Cases",
        "Trace32"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264482018",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264481160",
      "title": "Dietitian",
      "company": "Diet Speed",
      "location": "India",
      "listed_at": "",
      "application_type": "",
      "workplace_type": "Remote",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264481160",
      "company_url": "",
      "description": "üì¢ We‚Äôre Hiring: Dietitian (Work From Home) ü•óüíªJoin Diet Speed, a fast-growing health-tech company, and be part of a team that‚Äôs transforming lives through nutrition and lifestyle coaching!\nüïí Timings: 10:00 AM ‚Äì 7:30 PMüìç Location: Remote / Work From HomeüìÖ Experience Required: 6 months ‚Äì 1 yearüéì Qualification: M.Sc. in Food & Nutrition (Mandatory)üíº Role: Dietitian / Nutrition Consultant\nüî∏ Key Responsibilities:Conduct virtual consultations with clients.\nDesign personalized, goal-based diet plans.\nTake regular follow-ups to track client progress and address any challenges.\nEducate clients on healthy eating and sustainable lifestyle changes.\nCollaborate with internal teams to ensure excellent service delivery.\nMaintain accurate client records and progress reports.\nüåü Ideal Candidate:M.Sc. in Food & Nutrition (required)\n6 months to 1 year of practical experience\nExcellent communication skills in Hindi and English\nPassionate about wellness and helping people transform their health\nTech-savvy ‚Äì comfortable using Zoom, WhatsApp, Google Sheets, CRMs, etc.\nüì© To Apply: Send your resume to hr@dietspeed.co.in\nLet‚Äôs grow together and make India healthier, one life at a time! ‚ú®",
      "skills": [
        "Clinical Nutrition",
        "Consultations",
        "Diabetes",
        "Dietetics",
        "Food and Beverage Operations",
        "Hindi",
        "Medical Nutrition Therapy",
        "Nutrition",
        "Nutrition Education",
        "Nutritional Counseling"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266327541",
      "title": "Junior Software Engineer",
      "company": "SPORADA SECURE INDIA PRIVATE LIMITED",
      "location": "Chennai, Tamil Nadu, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4266327541",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266327541",
      "company_url": "",
      "description": "We are looking for a skilled Angular Developer to join our development team. You will be responsible for designing and building modern, responsive web applications using Angular. The ideal candidate should have a strong understanding of front-end technologies and be passionate about creating high-quality user interfaces.Key ResponsibilitiesDevelop and maintain web applications using Angular (v10+ or latest)Translate UI/UX designs into functional front-end codeIntegrate with RESTful APIs and backend servicesOptimize application for maximum speed and scalabilityWrite clean, maintainable, and testable codeTroubleshoot and debug application issuesWork closely with backend developers, UI/UX designers, and QA teamsParticipate in code reviews and Agile development processesRequired Skills & QualificationsStrong proficiency in Angular, TypeScript, JavaScript, HTML5, CSS3Experience with Angular CLI, RxJS, and state management (e.g., NgRx)Good understanding of RESTful APIs and JSONFamiliarity with Git, version control, and code collaboration toolsKnowledge of responsive and adaptive design principlesExperience with unit testing and end-to-end testing frameworks (e.g., Jasmine, Karma, Protractor)Preferred Skills (Nice to Have)Experience with other front-end frameworks or libraries (React, Vue.js)Knowledge of backend technologies (Node.js, Express, Firebase, etc.)Familiarity with CI/CD pipelinesExperience working with Agile/Scrum methodologies\n",
      "skills": [
        "Angular",
        "Back-End Web Development",
        "Cascading Style Sheets (CSS)",
        "Front-End Development",
        "Git",
        "JavaScript",
        "Object-Oriented Programming (OOP)",
        "Web Applications",
        "Graphic Design Principles",
        "HTML5"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4266327541",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266320682",
      "title": "Android Developer - SDE 1",
      "company": "Groww",
      "location": "Bangalore Urban, Karnataka, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4266320682",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266320682",
      "company_url": "",
      "description": "About GrowwWe are a passionate group of people focused on making financial services accessible to every Indian through a multi-product platform. Each day, we help millions of customers take charge of their financial journey. Customer obsession is in our DNA. Every product, every design, every algorithm down to the tiniest detail is executed keeping the customers‚Äô needs and convenience in mind. Our people are our greatest strength. Everyone at Groww is driven by ownership, customer-centricity, integrity and the passion to constantly challenge the status quo.Are you as passionate about defying conventions and creating something extraordinary as we are? Let‚Äôs chat.\nOur VisionEvery individual deserves the knowledge, tools, and confidence to make informed financial decisions. At Groww, we are making sure every Indian feels empowered to do so through a cutting-edge multi-product platform offering a variety of financial services. Our long-term vision is to become the trusted financial partner for millions of Indians.\nOur ValuesOur culture enables us to be what we are ‚Äî India‚Äôs fastest-growing financial services company. Everyone at Groww enjoys the autonomy and flexibility to bring their best work to the table, as well as craft a promising career for themselves.\nThe values that form our foundation are:Radical customer-centricityOwnership-driven cultureKeeping everything simpleLong-term thinkingComplete transparency\nWhat are we looking for:Bachelor's degree in Computer Science or a related discipline preferred.2-3 years of experience in Android mobile applications development with a sound understanding of Android Architecture, Framework, Android SDK, Core Java, Android Studio IDE, Android Debugger, Kotlin.Hands-on experience in building mobile applications and mobility solutions ‚Äì native applications.Expert in UI components & controls; Action Bars, Widgets, Fragments, Constraint layouts, etc.Expert in basic components of Android: Activity, Services, Intent, Broadcast Receiver, Content Provider, Handlers, Threads.Good working experience on Web Service Integration ( REST, JSON, XML) using Retrofit + OkHttp\nGood to have:Expertise in Kotlin such as coroutines, extension functions.Expertise in Rx java.Expertise in using view models and live data.Well versed in the latest updates in the android tech.No nonsense, clean architecture & good coding guidelines.Experience in using analytics, location and social APIs, payment gateway, SMS gateway, and cloud integration.",
      "skills": [
        "JSON",
        "RxJava"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4266320682",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266161571",
      "title": "Software Development Engineer II - Azure Data Engineering",
      "company": "Tesco Bengaluru",
      "location": "Bengaluru, Karnataka, India",
      "listed_at": "",
      "application_type": "https://careers.tesco.com/en_GB/careers/JobDetail/129454",
      "workplace_type": "",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266161571",
      "company_url": "",
      "description": "About the role\nWe are looking for a skilled Data Engineer to join our team, working on end-to-end data engineering and data science use cases. The ideal candidate will have strong expertise in Python or Scala, Spark (Databricks), and SQL, building scalable and efficient data pipelines on Azure.\nYou will be responsible for\nDesign, build, and maintain scalable ETL/ELT data pipelines using Azure Data Factory, Databricks, and Spark.Develop and optimize data workflows using SQL and Python or Scala for large-scale data processing and transformation.Implement performance tuning and optimization strategies for data pipelines and Spark jobs to ensure efficient data handling.Collaborate with data engineers to support feature engineering, model deployment, and end-to-end data engineering workflows.Ensure data quality and integrity by implementing validation, error-handling, and monitoring mechanisms.Work with structured and unstructured data using technologies such as Delta Lake and Parquet within a Big Data ecosystem.Contribute to MLOps practices, including integrating ML pipelines, managing model versioning, and supporting CI/CD processes.\nYou will need\nPrimary Skills:Data Engineering & Cloud:Proficiency in Azure Data Platform (Data Factory, Databricks).Strong skills in SQL and [Python or Scala] for data manipulation.Experience with ETL/ELT pipelines and data transformations.Familiarity with Big Data technologies (Spark, Delta Lake, Parquet).Data Optimization & Performance:Expertise in data pipeline optimization and performance tuning.Experience on feature engineering and model deployment.Analytical & Problem-Solving:Strong troubleshooting and problem-solving skills.Experience with data quality checks and validation.\nNice-to-Have Skills:Exposure to NLP, time-series forecasting, and anomaly detection.Familiarity with data governance frameworks and compliance practices.Basics of AI/ML like:ML & MLOps IntegrationExperience supporting ML pipelines with efficient data workflows.Knowledge of MLOps practices (CI/CD, model monitoring, versioning)\nWhats in it for you?\n\nAt Tesco, we are committed to providing the best for you. \n \nAs a result, our colleagues enjoy a unique, differentiated, market- competitive reward package, based on the current industry practices, for all the work they put into serving our customers, communities and planet a little better every day. \n \nOur Tesco Rewards framework consists of pillars - Fixed Pay, Incentives, and Benefits. \n \nTotal Rewards offered at Tesco is determined by four principles -simple, fair, competitive, and sustainable. \n \nSalary - Your fixed pay is the guaranteed pay as per your contract of employment. \n \nLeave & Time-off - Colleagues are entitled to 30 days of leave (18 days of Earned Leave, 12 days of Casual/Sick Leave) and 10 national and festival holidays, as per the company‚Äôs policy. \n \nMaking Retirement Tension-FreeSalary - In addition to Statutory retirement beneets, Tesco enables colleagues to participate in voluntary programmes like NPS and VPF. \n \nHealth is Wealth - Tesco promotes programmes that support a culture of health and wellness including insurance for colleagues and their family. Our medical insurance provides coverage for dependents including parents or in-laws. \n \nMental Wellbeing - We offer mental health support through self-help tools, community groups, ally networks, face-to-face counselling, and more for both colleagues and dependents. \n \nFinancial Wellbeing - Through our financial literacy partner, we offer one-to-one financial coaching at discounted rates, as well as salary advances on earned wages upon request. \n \nSave As You Earn (SAYE) - Our SAYE programme allows colleagues to transition from being employees to Tesco shareholders through a structured 3-year savings plan. \n \nPhysical Wellbeing - Our green campus promotes physical wellbeing with facilities that include a cricket pitch, football field, badminton and volleyball courts, along with indoor games, encouraging a healthier lifestyle. \n\nAbout Us\n\nTesco in Bengaluru is a multi-disciplinary team serving our customers, communities, and planet a little better every day across markets. Our goal is to create a sustainable competitive advantage for Tesco by standardising processes, delivering cost savings, enabling agility through technological solutions, and empowering our colleagues to do even more for our customers. With cross-functional expertise, a wide network of teams, and strong governance, we reduce complexity, thereby offering high-quality services for our customers. \n \nTesco in Bengaluru, established in 2004 to enable standardisation and build centralised capabilities and competencies, makes the experience better for our millions of customers worldwide and simpler for over 3,30,000 colleagues \n \nTesco Technology\n \nToday, our Technology team consists of over 5,000 experts spread across the UK, Poland, Hungary, the Czech Republic, and India. In India, our Technology division includes teams dedicated to Engineering, Product, Programme, Service Desk and Operations, Systems Engineering, Security & Capability, Data Science, and other roles. \n \nAt Tesco, our retail platform comprises a wide array of capabilities, value propositions, and products, essential for crafting exceptional retail experiences for our customers and colleagues across all channels and markets. This platform encompasses all aspects of our operations - from identifying and authenticating customers, managing products, pricing, promoting, enabling customers to discover products, facilitating payment, and ensuring delivery. By developing a comprehensive Retail Platform, we ensure that as customer touchpoints and devices evolve, we can consistently deliver seamless experiences. This adaptability allows us to respond flexibly without the need to overhaul our technology, thanks to the creation of capabilities we have built. ",
      "skills": [
        "Back-End Web Development",
        "JavaScript",
        "Object-Oriented Programming (OOP)",
        "Problem Solving",
        "Python (Programming Language)",
        "SQL",
        "Software Development",
        "Extract, Transform, Load (ETL)",
        "Optimization Strategies",
        "Performance Tuning"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://careers.tesco.com/en_GB/careers/JobDetail/129454",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264482161",
      "title": "Lead Databricks Data Architect",
      "company": "Celebal Technologies",
      "location": "India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264482161",
      "workplace_type": "Remote",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264482161",
      "company_url": "",
      "description": "Databricks Data Architect\nExperience: 10+ Years\nLocation: Hyderabad/Bengaluru/Mumbai/Pune/Noida/jaipur\nImmediate Joiners\nWe are seeking an experienced Databricks Data Architect with a strong background in designing scalable data platforms in the manufacturing or energy sector. The ideal candidate will have over 12-15 years of experience in designing and implementing enterprise-grade data solutions, with strong proficiency in Azure Databricks and big data technologies.\nKey Responsibilities:Architect and deliver scalable, cloud-native data solutions to support both real-time and batch processing needs.Work closely with business and technical stakeholders to understand business requirements, define data strategy, governance, and architecture standards.Ensure data quality, integrity, and security across platforms and systems.Define data models, data integration patterns, and governance frameworks to support analytics use cases.Collaborate with DevOps and Engineering teams to ensure robust CI/CD pipelines and deliver production-grade deployments.Define and enforce data architecture standards, frameworks, and best practices across data engineering and analytics teams.Implement data governance, security, and compliance measures, including data cataloguing, access controls, and regulatory adherence.Lead capacity planning and performance tuning efforts to optimize data processing and query performance.Create and maintain architecture documentation, including data flow diagrams, data models, entity-relationship diagrams, system interfaces etc.Design clear and impactful visualizations to support key analytical objectives.Required Skills and Experience:Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Information Technology, Engineering, or a related field.Strong proficiency in Azure Databricks and big data technologies (Apache Spark, Kafka, Event Hub).Deep understanding of data modeling, data lakes, batch and real-time/streaming data processing.Proven experience with high volume data pipeline orchestration and ETL/ELT workflows.Experience designing and implementing data lakes, data warehouses, and lakehouse architectures.Proven experience in designing and implementing data visualization solutions for actionable insights.Strong understanding of data integration patterns, APIs, and message streaming (e.g., Event Hub, Kafka).Experience with metadata management, and data quality frameworks.Excellent problem-solving skills and the ability to translate business needs into technical solutions.Experience with structured and unstructured data ingestion, transformation, and processing at scale.Excellent communication, documentation, and stakeholder management skills.Preferred Qualifications:Familiarity with lakehouse architectures using Delta Lake.Knowledge of manufacturing/energy domain-specific standards and protocols.Experience with IoT data and time-series analysis.Knowledge of data governance, security, and compliance best practices.",
      "skills": [
        "Apache Kafka",
        "Azure Data Factory",
        "Azure Databricks",
        "Data Architecture",
        "Data Integration",
        "Databricks Products",
        "Microsoft Azure"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264482161",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266331157",
      "title": "Sterling Developer mid level",
      "company": "Pragma Edge Inc",
      "location": "Hyderabad, Telangana, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4266331157",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266331157",
      "company_url": "",
      "description": "Company Description\n \nPragma Edge Inc. is a forward-thinking technology services provider dedicated to driving innovation and transformation across industries. Our mission is to empower businesses with cutting-edge solutions that enhance efficiency, foster growth, and solve complex challenges in today‚Äôs dynamic marketplace. We have grown into a trusted partner for companies in sectors such as Automotive, Healthcare, Insurance, Telecommunications, Energy & Utilities, Banking & Financial Services, and more. Our services include Application Integration Services, Partner Integration with MFT & B2Bi/EDI Services, Enterprise Asset Management, GenAI Services, Hyper Automation Services, Low Code Services, and Data & Analytics Services.\n\n Role Description\n \nThis is a full-time on-site role for a mid-level Sterling Developer located in Hyderabad. The Sterling Developer will be responsible for developing and maintaining Sterling B2B Integrator and Sterling File Gateway solutions. The role involves designing, coding, testing, and deploying integrations, as well as troubleshooting and supporting existing applications. The candidate will collaborate with cross-functional teams to gather requirements, develop technical specifications, and ensure seamless implementation of business requirements.\n\n Qualifications\n \nExperience in Sterling B2B Integrator and Sterling File Gateway development and maintenanceTechnical skills in Java, XML, XSLT, and database technologies like SQL, and DB2Knowledge of EDI, MFT (Managed File Transfer), and integration protocols (AS2, SFTP, FTPS)Proficiency in troubleshooting, performance tuning, and debugging integration solutionsStrong analytical and problem-solving skillsExcellent written and verbal communication skillsAbility to work independently and collaborate with cross-functional teamsBachelor's degree in Computer Science, Engineering, or related fieldRelevant certifications in IBM Sterling or related technologies are a plus",
      "skills": [
        "Back-End Web Development",
        "Computer Science",
        "FTPS",
        "File Transfer",
        "Performance Tuning",
        "Requirements Gathering",
        "Secure File Transfer Protocol (SFTP)",
        "Technical Specs",
        "Troubleshooting",
        "XML"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4266331157",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264465710",
      "title": "Senior AWS Data Engineer with Databricks Exp",
      "company": "Info Services",
      "location": "Hyderabad, Telangana, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264465710",
      "workplace_type": "Hybrid",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264465710",
      "company_url": "",
      "description": "Job Title: AWS Data Engineer (Databricks Focus)Location: Hyderabad - Hyderabad OfficeType: Full-Time\n\nPosition Overview:We are looking for an AWS Data Engineer with strong Databricks and PySpark experience to help design, develop, and optimize scalable data pipelines. The role requires a strong blend of data engineering expertise, AWS development experience, and a proactive mindset to innovate and lead technical efforts across the stack.\nKey Responsibilities:Develop and optimize data pipelines using Databricks (PySpark).Implement AWS AppSync and Lambda-based APIs for integration with Neptune and OpenSearch.Collaborate with React developers (AWS CloudFront) and backend teams to enhance architecture.Ensure secure development practices, especially around IAM roles and AWS security.Drive performance, scalability, and reliability improvements across the data ecosystem.Take full ownership of assigned tasks and deliverables.\nRequired Skills:Strong experience in Databricks and PySpark for building data pipelines.Proficient with AWS Neptune and OpenSearch.Hands-on experience with AWS AppSync and Lambda functions.Solid grasp of IAM, CloudFront, and API development in AWS.Familiarity with React.js front-end applications (a plus).Strong problem-solving, debugging, and communication skills.Ability to work independently and drive innovation.\nPreferred Qualifications:AWS Certifications (Solutions Architect, Developer Associate, or Data Analytics Specialty).Production experience with graph databases and search platforms.",
      "skills": [
        "Amazon Web Services (AWS)",
        "Data Analytics",
        "Data Engineering",
        "AWS Lambda",
        "Azure Databricks",
        "Data Pipelines",
        "Identity and Access Management (IAM)",
        "PySpark"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264465710",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266325966",
      "title": "Operations Support Analyst",
      "company": "Zensar Technologies",
      "location": "Bengaluru, Karnataka, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4266325966",
      "workplace_type": "Hybrid",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266325966",
      "company_url": "",
      "description": "Years of Experience (3-7 Years)\nPrimary Skill Need people who are providing support for the queries.More stronger on SQL side, will check on the issues on server etc. Other Roles & Responsibilities Browser technology and how to use the developer tools, Application knowledge infra structure understanding to do troubleshooting, good communication skillsConduct regular system checks. Diagnose and resolve technical problems, often using advanced diagnostic tools and troubleshooting methods.Provide technical assistance to developers to enhance features.Maintain detailed records of issues, resolutions, and user interactions to improve support processes.Work closely with other IT professionals and departments to ensure seamless integration and operation of IT systems.",
      "skills": [
        ".NET Framework",
        "ASP.NET",
        "Query Languages",
        "SQL",
        ".NET Core",
        "Azure SQL",
        "Customer Support",
        "Developer Tools",
        "IT Integration",
        "Technical Support",
        "Troubleshooting"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4266325966",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264469733",
      "title": "Staff Kubernetes Engineer",
      "company": "RemoteStar",
      "location": "Bengaluru, Karnataka, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264469733",
      "workplace_type": "Hybrid",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264469733",
      "company_url": "",
      "description": "At RemoteStar, we're currently hiring for one of our clients based in UK.\nAbout client : A FTSE 250 global fintech company headquartered in London with a presence in 18 countries and five continents. Their award-winning products and platforms empower go-getters around the world giving them access to over 19,000 financial markets.\nYour role :Design, deploy, and manage containerized applications using EKS, ECS, and Nomad.\nImplement infrastructure as code (IaC) using Terraform for provisioning and managing cloud infrastructure.\nOptimize Kubernetes clusters for performance, cost, and security.\nDefine and enforce best practices for container security, networking, and observability\nImplement auto-scaling solutions using Karpenter, Cluster Autoscaler, or custom scaling policies.\nSupport multi-cloud and hybrid cloud container strategies.\nConduct performance benchmarking and capacity planning.\nTroubleshoot and resolve complex issues related to container orchestration and workload performance.\nCollaborate with developers, DevOps, and SRE teams to enhance the container platform.\nExperience with service mesh solutions (Istio, Consul, Linkerd) and container networking.\nHands-on experience with GitOps tools (ArgoCD, Flux) and Helm.\nManage the containerisation environments with tools such as Terraform and Terraform CDK and others.\nWork closely with the Cloud Engineering team.\nWhat you‚Äôll do :Kubernetes Platform Engineer should design and automate the process with proper tools. \nBuild Manage and maintain multitenant EKS/Nomad clusters \nDeveloping appropriate DevOps channels throughout the organization. \nEvaluating, implementing and streamlining DevOps practices. \nEstablishing a continuous build environment to accelerate software deployment and development processes. \nEngineering general and effective processes.\nHelping operation and development teams to solve their problems. \nHandling cloud-based environments - Amazon Web Services (AWS). \nHandling automated deployment CI/CD tools \nSupervising, Examining and Handling technical operations. \nProviding a DevOps Process and Operations. \nCapacity to handle teams with leadership attitude.\nGreat focus on developer experience.\nWork in an international multicultural environment.\nWhat you‚Äôll need for this role :14 + years of experience of which at least 5 years in design and administering complex production Kubernetes platforms. (CKA and or CKAD certifications desired)\n4+ years of experience in Infrastructure As Code (Terraform/Ansible/CDK/Pulumi).\nExperience with Hashicorp Nomad.\nMust have DevSecOps skills required (CI\\CD, Terraform, Ansible, Gitlab,Artifactory, Git ,etc.) \nSolid working knowledge of networking technologies.\nSolid working knowledge of AWS or GCP.\nGood understanding on the IaaS, PaaS layers.\nDrive initiatives to identify right set of cloud technologies/ provider to be used for specific type of solutions and drive the effort to templatize and automate those technologies to be used at scale in all products and applications across the organization. \nDefine and implement the relevant monitoring and alerting. \nWrite and maintain technical documentation, for internal use and for other stakeholders. \nExperience on leading the design and architecting of Kubernetes ecosystem, including competing technologies (deployment, configuration, scaling and management of containerized applications). \nExperience in building secure and optimized docker images.\nExperience with managed Kubernetes services, EKS (preferred) or GKE. \nBash or any scripting language and preferably Python or Go.\nCertified Kubernetes Administrator (CKA) or AWS Certified Kubernetes Specialist. (preferred)",
      "skills": [
        "Amazon Web Services (AWS)",
        "Continuous Integration and Continuous Delivery (CI/CD)",
        "Kubernetes",
        "Ansible",
        "Artifactory",
        "DevSecOps",
        "Gitlab",
        "HashiCorp",
        "Nomad",
        "Terraform"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264469733",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264478093",
      "title": "Back End Developer",
      "company": "CodeVyasa ",
      "location": "Gurugram, Haryana, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264478093",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264478093",
      "company_url": "",
      "description": "Looking for Backend Developer | Gurgaon to join a team of rockstar developers. The candidate should have a minimum of 5 yrs. of experience.\nThere are multiple openings. If you're looking for career growth & a chance to work with the top 0.1% of developers in the industry, this one is for you! You will report into IIT'ans/BITS grads with 10+ years of development experience + work with F500 companies (our customers).\nCompany Background - We are a multinational software company that is growing at a fast pace. We have offices in Florida & New Delhi. Our clientele spreads across the US, Australia & APAC. Here's the link to our website (codevyasa.com). To give you a sense of our growth rate, we've added 70+ employees in the last 6 weeks itself and expect another 125+ by the end of Q1 2025.\nKey Responsibilities:Design, develop, test, and maintain backend services using Python (Django/Flask/FastAPI) and Node.js (Express/NestJS).Build and maintain RESTful and/or GraphQL APIs.Integrate and manage relational and non-relational databases (PostgreSQL, MongoDB).Collaborate with front-end developers, DevOps, and product teams to build and deliver features end-to-end.Write clean, modular, and reusable code with proper documentation and test coverage.Optimize application performance and scalability.Participate in code reviews, design discussions, and agile development processes.Required Skills:Strong proficiency in Python and one or more frameworks: Django, Flask, or FastAPI.Solid experience with Node.js, especially using Express.js or NestJS.Expertise in PostgreSQL and MongoDB, including schema design and optimization.Experience building RESTful APIs; GraphQL knowledge is a plus.Familiarity with version control tools (e.g., Git) and CI/CD pipelines.Good understanding of backend architecture, microservices, and containerization concepts (Docker is a plus).Ability to write unit and integration tests.\nHere is what we have on offer for youGlassdoor's rating of 4.8Free healthcareLaser-sharp focus on upskilling our employeesDiverse & Inclusive teamsIndustry-par compensation & benefitsGreat work-life balance",
      "skills": [
        "Django",
        "MongoDB",
        "Node.js",
        "PostgreSQL",
        "Python (Programming Language)",
        "NestJS"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264478093",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264473880",
      "title": "Oracle Cloud Technical ",
      "company": "People Consultancy Services (PCS)",
      "location": "India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264473880",
      "workplace_type": "Remote",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264473880",
      "company_url": "",
      "description": "Position: Oracle Cloud Technical Location - Gurugram, Kolkata, Mumbai, Chennai, Hyderabad, Bangalore, Pune (Hybrid ‚Äì 2 days on-site, 3 days remote)Duration: Permanent Role\nDescription Candidate should have atleast 5 - 8 years of experienceShould have one or more project implementation experience on Cloud ERP Expertise in web services: Developing and consuming REST and SOAP based services Consuming Oracle Cloud Services including ERP Integration Service, External Report Service, HCM Data Loader, Generic Soap Port, and other common Cloud services Web service security Should have expertise in one of the following areas:Customization and Integrations using PaaS technologies such as JCS, JCS-SX, ADF, OAF, OAIC, OIC, ICS etc.Should have expertise in Data conversions using FBDI Templates and Rapid Implementation Sheets with the ability treconcile the data and correct it using ADFDi Should have expertise in developing BIP Reports, OTBI Reports, SmartView Reports and FRS ReportsShould have expertise in creating and scheduling Oracle Cloud ESS Jobs and BIP Jobs Should have experience in the Integration Patterns in Cloud Should have an exposure tonsite/offshore model in order gather requirements, design the solution and create technical Specification for individual requirementsShould know about cloud configuration and setup, Application Composer and Page Composer tbuild UI ExtensionsShould know about Oracle SaaS Security including privileges, roles and data accessShould know on how tmigrate SaaS configuration(DFF, Lookups and other related functional setups)Should know best practices of migrating PaaS Solution including DBCS PLSQL/SQL codes, ICS Integrations, SOA Composite deployments and others Knowledge on ADF, OAF, MAF and Oracle EBS on-premise will be an added advantageShould possess excellent verbal and written communication skills",
      "skills": [
        "Oracle Cloud"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264473880",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264476359",
      "title": "Application Architect - Java, .Net, Python",
      "company": "UPS",
      "location": "Chennai, Tamil Nadu, India",
      "listed_at": "",
      "application_type": "https://www.jobs-ups.com/imea/en/job/UPBUPSGLOBALR25011934EXTERNALENIMEA/Application-Architect-Java-Net-Python?utm_source=linkedin&utm_medium=phenom-feeds",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264476359",
      "company_url": "",
      "description": "Before you apply to a job, select your language preference from the options available at the top right of this page.\n\nExplore your next opportunity at a Fortune Global 500 organization. Envision innovative possibilities, experience our rewarding culture, and work with talented teams that help you become better every day. We know what it takes to lead UPS into tomorrow‚Äîpeople with a unique combination of skill + passion. If you have the qualities and drive to lead yourself or teams, there are roles ready to cultivate your skills and take you to the next level.\n\nJob Description\n\nJob Summary\n\nThis position influences the development and implementation of Information Technology (I.T.) strategy, initiatives, and governing policies. He/She assembles detailed reviews of the enterprise and documents capabilities and conceives approaches to aligning technical solutions with business needs. This position assists in defining the direction for projects and solution architecture. This position plans and champions the execution of broad initiatives aimed at delivering value to internal and external stakeholders. He/She leverages data, technical, and business knowledge to drive the development of capability frameworks at portfolio and enterprise levels. This position is involved throughout the project life cycle with emphasis on the initiation, feasibility, and analysis phases.\n\nResponsibilities\n\nInfluences the development and implementation of Information Technology (I.T.) strategy, initiatives, and governing policies.Assembles detailed reviews of the enterprise and documents capabilities and conceives approaches to aligning technical solutions with business needs.Assists in defining the direction for projects and solution architecture.Supports I.T. leadership by planning and championing the execution of broad initiatives aimed at delivering value to internal and external stakeholders.Leverages data, technical, and business knowledge to drive the development of capability frameworks at portfolio and enterprise levels.Supports project life cycle with emphasis on the initiation, feasibility, and analysis phases.\n\nQualifications\n\n5+ years‚Äô experience in Content Management Development ProjectsExperience in defining new architectures and ability to drive an independent project from an architectural stand pointExperience with developing software solutions & web servicesUnderstanding of Information Security practices, database design principles, cloud base solutions, UPS.com systems & business capabilitiesExperience in DevOps and Agile Strong negotiation skillsStrong written and verbal communication skills Experience with OpenText, TeamSite/LiveSite and Adobe AEM - PreferredBachelor's Degree or international equivalent in Computer Science or related field - Preferred\n\nEmployee Type\n\nPermanent\n\nUPS is committed to providing a workplace free of discrimination, harassment, and retaliation.",
      "skills": [
        ".NET Framework",
        "Computer Science",
        "Java",
        "Software Development",
        "Web Services",
        "Data Architecture",
        "Graphic Design Principles",
        "Management Development",
        "Real Estate Development Projects",
        "Solution Architecture"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.jobs-ups.com/imea/en/job/UPBUPSGLOBALR25011934EXTERNALENIMEA/Application-Architect-Java-Net-Python?utm_source=linkedin&utm_medium=phenom-feeds",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264476369",
      "title": "Graphic Designer",
      "company": "Kazam",
      "location": "Bengaluru, Karnataka, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264476369",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264476369",
      "company_url": "",
      "description": "About Kazam:  Kazam is leading India's EV charging revolution with an agnostic software platform and extensive charging station network. We are looking for a creative Graphic Design to help shape our visual identity and support our marketing and product design initiatives.\nKey Responsibilities:‚óè Design Engaging Content: Create visuals for social media, marketing materials, and print/digital platforms.‚óè Brand Development: Maintain and evolve Kazam‚Äôs visual identity, ensuring consistency across channels.‚óè Collaboration: Work with cross-functional teams and integrate feedback to refine designs.‚óè Stay Updated: Keep up with design trends and apply innovative ideas to Kazam's projects.\nQualifications:‚óè Pursuing or recently completed a degree in Graphic Design, Visual Communication, or related field.‚óè Proficient in Adobe Creative Suite (Photoshop, Illustrator, InDesign).‚óè Strong understanding of design principles, color theory, and typography.‚óè Excellent communication and attention to detail.\nWhy Join Us? Be part of a team shaping the future of sustainable transportation. Contributeyour design skills to a mission-driven company making a positive environmental impact.",
      "skills": [
        "Abstracting",
        "Adobe Creative Suite",
        "Adobe InDesign",
        "Creativity Skills",
        "Design",
        "Graphic Design",
        "Graphics",
        "Illustrative",
        "Logo Design",
        "Typography"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264476369",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264464785",
      "title": "Odoo Developer",
      "company": "Maxima Apparel",
      "location": "Gurugram, Haryana, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264464785",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264464785",
      "company_url": "",
      "description": "As an Odoo Developer at Maxima Apparel, you will play a key role in the design, customization, and implementation of Odoo modules, tailored to meet our growing organizational needs. You will work closely with cross-functional teams to ensure high-quality deliverables and the smooth operation of our ERP system, all while driving innovation in our workflows.\nShift Timing: Full-time (onsite) Monday-Friday 06:30pm-3:30pm ISTLocation: Gurgaon, India\nKey Responsibilities:Lead the design and development of custom Odoo modules to enhance ERP software and workflow processes.Customize and optimize Odoo applications based on specific business requirements.Implement new features, functionalities, and integrations with third-party applications.Troubleshoot technical issues, ensuring the reliability and performance of Odoo systems.Plan and execute data migration from legacy systems to Odoo.Collaborate with cross-functional teams to gather requirements and deliver tailored solutions.Support the data mapping, architecture, and integration of Odoo modules into existing systems.Develop and run comprehensive test plans to ensure the integrity of Odoo applications.Maintain clear and concise technical documentation for future reference.\nQualifications:\nExperience:2-3 years of experience in Odoo development.Proven experience in Python, XML, JavaScript, and PostgreSQL/MySQL.Prior experience in ERP system development and implementation.\nSkills:Deep understanding of Odoo architecture and various modules.Strong analytical and problem-solving abilities.Effective communication skills to collaborate with various teams and stakeholders.Experience with APIs and third-party integration tools.Leadership qualities with a collaborative mindset.\nWhy Maxima Apparel?Work-life Balance: We value flexibility and believe in a balanced work environment.Learning & Development: Access to continuous learning opportunities to enhance your skills.Career Growth: With our rapid expansion, you‚Äôll have the chance to grow and take on new responsibilities.Work Culture: Be part of a collaborative and inclusive team, where your contributions are valued.\nAbout Maxima Apparel:Maxima Apparel Corp is a leading collective of sportswear and licensed apparel brands. We specialize in delivering high-quality men's and women's licensed apparel, outerwear, and headwear. With a commitment to fast, agile manufacturing, we serve some of the biggest names in the business, ensuring top-tier quality, competitive pricing, and outstanding customer service.\nJoin Maxima Apparel as we continue to set new standards for ERP and data-driven excellence.",
      "skills": [
        "Application Development",
        "Computer Engineering",
        "Computer Science",
        "PostgreSQL",
        "Programming",
        "Python (Programming Language)",
        "Query Languages",
        "Relational Databases",
        "Web Applications",
        "ERP Software",
        "Infor Enterprise Resource Planning (ERP)",
        "Open XML",
        "SQL Database Administration"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264464785",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266328581",
      "title": "Senior Full Stack Java Developer (946)",
      "company": "DBSync",
      "location": "Bengaluru, Karnataka, India",
      "listed_at": "",
      "application_type": "https://apply.workable.com/j/305E4EC68A",
      "workplace_type": "Remote",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266328581",
      "company_url": "",
      "description": "About the Role:We are looking for an experienced Full Stack Java Developer to be responsible for providing solutions for technical issues which may affect product delivery. The Full Stack Developer will facilitate requirement analyses, conduct peer reviews and provide feedback, and enhance frameworks.The goal is to provide a framework for the development of a software or system that will result in high quality IT solutions.\nYouAre passionate about technology, wake up every morning to experiment, learn and develop new solutions!\nYou WillWork on latest edge Cloud computing to solve difficult, manual and repeatable tasks for users adopting Cloud technologies like AWS, Salesforce, Microsoft and moreUse the latest trends on building SaaS applications using multitenancy, scalability large data volumes / Big Data and moreDevelop and expand our portfolio to support most popular Cloud apps (200+)Develop understanding of not just software development, but also how to design, develop and launch a product from concept to high customer use.\nAbout Us:At DBSync we provide opportunity to use technology that creates innovative and next generation data integration products which helps our employees to cultivate their creativity and sense of belonging. DBSync is a leading provider for data integration and Data warehousing services for both cloud/SaaS-based-on-demand applications. We were incubated by Salesforce, before getting spunned off as a separate company. DBSync provides data integration as a SaaS-based, Standalone or hosted integration at a competitive price with a plethora of benefits for business users.Our main motto is to provide a strong foundation for businesses in solving complex problems and automating their workflows while acting as a pillar of support for all their data integrations needs. At DBSync we encourage innovation and creativity and our team is diverse comprising of a nice mix of cultural blends. We are a family where each talent is recognized and honed for the best of the individual and the company as a whole. We maintain a very professional, learn and excel vision with a hint of a start-up-like environment making each individual feel important and responsible with an equal opportunity to grow to the sky's limit!If you have a spirit of enthusiasm to grow beyond horizons, a determination to work hard, confidence to achieve success and influence future visions and products and be a successful part of the next gen trend, DBSync is the place for you!\nOur Value System1. We genuinely care2. We do not waste time on manual or mundane task3. We have a fun environment4. We own it5. We are experts on what we do6. We win together\nDBSync has been:We have been rated 4.7 / 5 on G2. We have got around 125 reviews in 2023.Based on the reviews G2 has awarded DBSync following badges:-Best Relationship- Winter 2024-Leader Americas Winter 2024-Best Results - Winter 2024-Best Usability - Winter 2024\nJob Responsibilities: Performing requirement analyses. Developing high-quality and detailed designs. Conducting unit testing using automated unit test frameworks. Identifying risk and conducting mitigation action planning. Conducting configuration of your own work. Developing and reviewing the work of other developers and providing feedback. Using coding standards and best practices to ensure quality. \nSkills and Qualifications Required: Proven experience as Software Engineering and Technical Lead Must have strong coding skills Experience in software development and coding in various languages (Java, Linux, AWS) Extensive experience in the IT industry Hands-on with Bootstrap, JavaScript and UI/UX design Understanding of software quality assurance principles A technical mindset with great attention to detail Excellent analytical skills Bachelors in computer science, engineering or relevant field \nWhy Choose DBSync?DBSync goes beyond offering just a job; we provide a platform for your career growth, learning, and success.\nHere's why you should consider joining our team:Growth Opportunities: DBSync offers a wide range of opportunities for career advancement and professional development. Whether you're an experienced professional or just starting out, we provide programs and resources to help you achieve your full potential.Collaborative Environment: We believe in the strength of collaboration at DBSync. Our inclusive and diverse workplace encourages employees to share ideas, work together on projects, and support each other in reaching common objectives.Leading Training Programs: We invest in our employees' success by providing access to top-notch training and development initiatives. Whether you're looking to enhance your technical skills or leadership abilities, we offer resources to keep you ahead in your field.Comprehensive Benefits: Along with competitive pay, DBSync provides a comprehensive benefits package that includes health insurance, retirement plans, paid time off, and more.\nWork Location: BangaloreWork Shift: U.K Shift (01:30 PM IST to 10:30 PM IST )\nFor more information about DBSync, visit https://www.mydbsync.comVideo: https://www.youtube.com/playlist?list=PLJzycdoERLoekPWNkXEsuih807TQAEnnB\nBenefits\n Private Health Insurance Pension Plan Paid Time Off Work From Home Training & Development Performance Bonus",
      "skills": [
        "Amazon Web Services (AWS)",
        "Computer Science",
        "Full-Stack Development",
        "Java",
        "JavaScript",
        "Software Development",
        "Attention to Detail",
        "Bootstrap (Framework)",
        "Linux",
        "Stack"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://apply.workable.com/j/305E4EC68A",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264478280",
      "title": "SDET",
      "company": "Health Catalyst",
      "location": "Chandigarh, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264478280",
      "workplace_type": "Remote",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264478280",
      "company_url": "",
      "description": "About us :The healthcare industry is the next great frontier of opportunity for software development, and Health Catalyst is one of the most dynamic and influential companies in this space. We are working on solving national-level healthcare problems, and this is your chance to improve the lives of millions of people, including your family and friends. Health Catalyst is a fast-growing company that values smart, hardworking, and humble individuals. Each product team is a small, mission-critical team focused on developing innovative tools to support Catalyst‚Äôs mission to improve healthcare performance, cost, and quality.Health Catalyst is expanding and maintains a large suite of Improvement Apps that contribute to healthcare analytics and process improvement solutions. This includes products that manage the care of health system populations, better serve patients at the point of care, reduce health system costs, and reduce clinician workload.\nJob Description:We‚Äôre seeking a skilled SDET with hands-on experience in Playwright automation using JavaScript/TypeScript and a solid understanding of performance testing. You‚Äôll build and maintain automated test frameworks, collaborate with cross-functional teams, and help ensure high-quality, performant software releases.\nResponsibilities:Develop and maintain automated test scripts for web and API testing using Playwright (JS/TS).Build scalable, maintainable automation frameworks following best practices.Integrate automated tests into CI/CD pipelines (e.g., Azure DevOps).Collaborate with developers, QA, and product teams to define test strategies and plans.Troubleshoot automation issues and identify root causes.Conduct exploratory testing to complement automation coverage.Support performance testing initiatives using tools like JMeter or LoadRunner.Participate in Agile ceremonies and code reviews to uphold quality standards.Continuously improve testing processes and frameworks.Qualifications:3+ years of automation experience, primarily with Playwright in JavaScript/TypeScript.Experience with API testing tools (e.g., Postman).Familiarity with performance testing tools and methodologies is a strong plus.Solid understanding of various testing types: regression, system, acceptance, load, performance.Experience in Agile/Scrum environments.Proficient in SQL and database testing.Knowledge of CI/CD tools and version control (e.g., Azure DevOps, Git).Familiarity with containerization (Docker/Kubernetes) is desirable.Strong analytical, communication, and problem-solving skills.Self-motivated and eager to learn new technologies.",
      "skills": [
        "API Testing",
        "Continuous Integration and Continuous Delivery (CI/CD)",
        "JavaScript",
        "Programming",
        "SQL",
        "TypeScript",
        "Azure DevOps Services",
        "Playwriting"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264478280",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264466820",
      "title": "Redwood Developer",
      "company": "Schneider Electric",
      "location": "Bangalore Urban, Karnataka, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4264466820",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264466820",
      "company_url": "",
      "description": "Experience:‚Ä¢ Should have at least 4 to 7 years of experience in Redwood Administration. Job Description: As a Redwood Application Admin, you will configure applications to meet business process and application requirements. You will play a crucial role in ensuring the smooth functioning of applications and their alignment with business needs, working closely with the Design and Engineering teams. Roles and Responsibilities:‚û¢ Responsible for Redwood RunMy Finance server setup, installation of Redwood Agents, and related configuration on the Redwood platform to integrate with Schneider ERPs.‚û¢ Must have advanced knowledge to understand and execute Java code to maintain custom solutions within Redwood.‚û¢ Experience in upgrading custom programs to enable required finance processes.‚û¢ Experience in troubleshooting issues related to configurations.‚û¢ Responsible for batch job scheduling through Redwood based on client requests and dependencies. Also, monitor and manage scheduled jobs to track job execution and troubleshoot issues.‚û¢ Perform system upgrades and patches to ensure the system is up-to-date. Monitor and optimize system performance.‚û¢ Conduct end-user training for new deployments.‚û¢ Ensure archival of Redwood processes and components as required by the organization.‚û¢ Knowledge of promoting chains and processes from one Redwood environment to another, ensuring consistency across environments.‚û¢ Conduct daily system health checks and system cleanup activities to ensure system performance.‚û¢ Monitor and resolve priority tickets/incidents created by end users, ensuring resolution within the required SLA.‚û¢ Experience in handling tickets using the ServiceNow ticketing tool.‚û¢ Complete knowledge of REL expressions in Redwood. Primary & Secondary Skillset:‚ùñ Good knowledge of the technical/platform setup of Redwood Finance Automation and its interfaces and connectors.‚ùñ Good knowledge of security concepts and authorization setup (roles and privileges), interfaces towards Identity Management Systems, and Active Directory/LDAP for authentication.‚ùñ Good knowledge of different operating systems like Windows and Unix/Linux.‚ùñ Advanced knowledge of MS Excel.‚ùñ Computer science background.‚ùñ Basic finance functional knowledge.‚ùñ SAP S4HANA cloud certification (added advantage).‚ùñ Knowledge or experience in integrating SAP ECC S/4 HANA cloud with Redwood.‚ùñ Experience or understanding of SaaS solutions and integrations.‚ùñ Ability to contribute and collaborate well with diverse teams to deliver business objectives. Soft Skills:Excellent communication and presentation skills (written and verbal).Proven track record in successful teamwork as part of global, multi-national projects.Ability to explain technical information to non-technical people.Multi-cultural awareness, open-minded to working in diverse business environments.Able to constructively work under stress and pressure when faced with high workloads and deadlines. For More Details, Please connect with following persons1. Ganesh",
      "skills": [
        "BSR Advance",
        "Job Scheduling",
        "Redwood",
        "SAP ERP",
        "Software as a Service (SaaS)"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4264466820",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264473288",
      "title": "Data Engineer",
      "company": "InfoVision Inc.",
      "location": "Pune, Maharashtra, India",
      "listed_at": "",
      "application_type": "https://infovision.openings.co/#!/job-view/data-engineer-pune-2025063015332563?source=linkedin",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264473288",
      "company_url": "",
      "description": "Critical Skills To Possess\n\nAdvanced working knowledge and experience with relational and non-relational databases.Advanced working knowledge and experience with API data providersExperience building and optimizing Big Data pipelines, architectures, and datasets.Strong analytic skills related to working with structured and unstructured datasets.Hands-on experience in Azure Databricks utilizing Spark to develop ETL pipelines.Strong proficiency in data analysis, manipulation, and statistical modeling using tools like Spark, Python, Scala, SQL, or similar languages.Strong experience in Azure Data Lake Storage Gen2, Azure Data Factory, Databricks, Event Hub, Azure Synapse.Familiarity with several of the following technologies: Event Hub, Docker, Azure Kubernetes Service, Azure DWH, API Azure, Azure Function, Power BI, Azure Cognitive Services.Azure DevOps experience to deploy the data pipelines through CI/CD.\n\nPreferred Qualifications\n\nBS degree in Computer Science or Engineering or equivalent experience",
      "skills": [
        "Apache Spark",
        "Big Data",
        "Computer Science",
        "Data Analytics",
        "Data Engineering",
        "Data Science",
        "Python (Programming Language)",
        "Data Warehousing",
        "Extract, Transform, Load (ETL)",
        "Microsoft Azure"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://infovision.openings.co/#!/job-view/data-engineer-pune-2025063015332563?source=linkedin",
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266325862",
      "title": "Architect - Data Engineering [T500-19153]",
      "company": "ANSR",
      "location": "Hyderabad, Telangana, India",
      "listed_at": "",
      "application_type": "https://www.linkedin.com/job-apply/4266325862",
      "workplace_type": "On-site",
      "employment_type": "",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266325862",
      "company_url": "",
      "description": "ANSR is hiring for one of its clients.About Us:ArcelorMittal was formed in 2006 from the strategic merger of European company Arcelor and Indian-owned Mittal Steel. Over a journey of two decades, we have emerged as the world's leading steel and mining company, exerting our influence across 60+ countries with a robust industrial footprint in 18. We are a global team of 158,00+ talented individuals committed to building a better world with smarter low-carbon steel. Our strategies are not just about scale; they're also about leading a transformative change where innovation meets sustainability. We supply to major global markets‚Äîfrom automotive and construction to household appliances and packaging‚Äîsupported by world-class R&D and distribution networks.ArcelorMittal Global Business and Technologies in India is our new hub of technological innovation and business solutions. Here, you'll find a thriving community of business professionals and technologists who bring together diverse and unique perspectives and experiences to disrupt the global steel manufacturing industry. This fusion ignites groundbreaking ideas and unlocks new avenues for sustainable business growth. We nurture a culture fueled by an entrepreneurial spirit and a passion for excellence, which prioritizes the advancement and growth of our team members. With flexible career pathways and access to the latest technology and business tools, we offer a space where you can learn, take ownership, and face exciting challenges every day.\nPosition Summary:IT - Architect ‚Äì Azure Lake - D&IT DATA\nJob Responsibility:Strategic Data Architecture and Roadmap:Develop and maintain the company‚Äôs data architecture strategy aligned with business objectives. Lead design and/or architecture validation reviews with all stakeholders, assess projects against architectural principles and target solutions, organize and lead architecture committees. Select new solutions that meet business needs, aligned with existing recommendations and solutions, and broadly with IS strategy. Model the company‚Äôs information systems and business processes. Define a clear roadmap to modernize and optimize data management practices and technologies. \nPlatform Design and Implementation:Architect scalable data flows, storage solutions, and analytics platforms in cloud and hybrid environments. Ensure secure, high-performing, and cost-effective data solutions. \nData Governance and Quality:Establish data governance frameworks ensuring data accuracy, availability, and security. Promote and enforce best practices for data quality management. Ensure compliance with enterprise architecture standards and principles. \nTechnical Leadership:Act as a technical advisor on complex data-related projects and proof of concepts. \nStakeholder Collaboration:Collaborate with business stakeholders to understand data needs and translate them into architectural solutions. Work with relevant stakeholders in defining project scope, planning development, and validating milestones throughout project execution. \nEmerging Technologies and Innovation:Drive the adoption of new technologies and assess their impact on the organization‚Äôs data strategy. Conduct technological watch in both company activity domains and IT technologies, promoting innovative solutions adapted to the company. Define principles, standards, and tools for system modeling and process management. \nMay support a wide range of technologies related to Datalakes:Development: SQL, SYNAPSE, Databricks, PowerBi, Fabric Tools: Visual Studio & TFS, GIT. Database: SQL Server. Methodologies: Agile (SCRUM). SAP BW / SAC \nRequired Skill:Azure or AWS, Databricks and Synapse Deep knowledge of cloud data platforms (Microsoft Azure, Fabric, Databricks). Understanding of SAP-based technologies (SAP BW, SAP DataSphere, HANA, S/4, ECC). Experience with visualization, reporting, and self-service tools (Power BI, Tableau, SAP Analytics Cloud). Understanding data modeling, ETL / ELT technologies, and big data. Experience with relational and NoSQL databases. Deep knowledge of data security and compliance best practices Experience with AI / ML concepts and technologies.Experience in designing and implementing AI solutions within data architectures.Proven ability to lead AI / ML projects from conception to deployment.Familiarity with data mesh and data fabric architectural approaches. \nQualification and Experience:Experience ‚Äì 6-8 years Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Data Science, or a related field. Experience in data architecture, with at least 3 years in a leadership role. Experience with AI / ML projects is highly preferred. Certifications in data architecture or cloud technologies. Certifications in project management. 3-year experience as Analyst in large scale projects. 5-year experience in back-end / full stack development in large scale projects with Azure Synapse / Databricks Excellent communication and presentation skills for both technical and non-technical audiences. Strong problem-solving skills and an ability to navigate complexity.",
      "skills": [
        "Microsoft Azure"
      ],
      "benefits": [],
      "seniority_level": "",
      "job_functions": [],
      "industries": [],
      "apply_url": "https://www.linkedin.com/job-apply/4266325862",
      "search_role": "Machine Learning Engineer"
    }
  ]
}