{
  "search_metadata": {
    "total_jobs": 20,
    "search_date": "1752512573.6577303",
    "roles_searched": [
      "Machine Learning Engineer",
      "Machine Learning Scientist",
      "ML Engineer",
      "ML Scientist",
      "Artificial Intelligence Engineer",
      "AI Engineer",
      "AI Scientist",
      "Data Scientist",
      "Applied Scientist",
      "Research Scientist",
      "ML Research Engineer",
      "AI Research Engineer",
      "Deep Learning Engineer",
      "Computer Vision Engineer",
      "NLP Engineer",
      "Natural Language Processing Engineer",
      "Computer Vision Scientist",
      "NLP Scientist",
      "Deep Learning Scientist",
      "MLOps Engineer",
      "ML Platform Engineer",
      "AI Platform Engineer",
      "Senior Machine Learning Engineer",
      "Senior ML Engineer",
      "Senior AI Engineer",
      "Lead Machine Learning Engineer",
      "Lead ML Engineer",
      "Lead AI Engineer",
      "Principal Machine Learning Engineer",
      "Principal ML Engineer",
      "Principal AI Engineer",
      "Machine Learning Developer",
      "AI Developer",
      "ML Developer",
      "Machine Learning Specialist",
      "AI Specialist",
      "ML Specialist",
      "Machine Learning Architect",
      "AI Architect",
      "ML Architect",
      "Machine Learning Researcher",
      "AI Researcher",
      "ML Researcher",
      "Research Engineer",
      "Applied Research Scientist",
      "Machine Learning Research Engineer",
      "AI Research Scientist",
      "ML Software Engineer",
      "AI Software Engineer",
      "Machine Learning Software Engineer",
      "AI/ML Engineer",
      "ML/AI Engineer",
      "Machine Learning & AI Engineer",
      "AI & Machine Learning Engineer"
    ],
    "search_location": "Global",
    "jobs_per_role": 30,
    "max_total_jobs": 20,
    "seconds_back": 3600,
    "days_back": 0,
    "detailed_job_info": true,
    "job_skills_info": true
  },
  "jobs": [
    {
      "id": "4266546184",
      "title": "Agile Release Train Engineer - AI",
      "company": "Red Hat",
      "location": "Bengaluru, Karnataka, India",
      "workplace_type": "",
      "experience_level": "",
      "job_url": "https://redhat.wd5.myworkdayjobs.com/jobs/job/Bangalore---Carina/Agile-Release-Train-Engineer---AI_R-049057-2?%26%3Fmode=job&%26iis=Job%2BBoard&%26iisn=LinkedIn%2BPosting&source=LinkedIn",
      "description": "The Red Hat Information Technology organization is looking for an Agile Release Train Engineer (RTE) to support our expanding AI capabilities and resources. In this role, you’ll be supporting the Data & AI team, Core Business Platforms, and the Planning Insights & Execution team as they collaborate for holistic planning and delivery to support the operational and product functions of Red Hat.\n\nYou will have the opportunity to join a highly cross functional and collaborative team and contribute leadership and subject matter expertise to areas requiring a multidisciplinary perspective. As an RTE you will be responsible for leading the Scaled Agile (SAFe) practice and maturity across the teams that support AI disciplines. You will be responsible for driving delivery and dependencies for initiatives that cross the AI space as well as coaching the continued adoption of the SAFe op model. We’ll need you to be motivated to engage with delivery leaders, stakeholders, Product Managers, and process owners to guide and enable their success, agile in response to changes and needs of various partners, and analytical in identifying risks, opportunities and dependencies across teams.\n\nWhat Will You Do\n\nWork in partnership with Data & AI leads to proactively manage key deliverables for all technical work streams in addition to managing dependencies and risksInstill and drive maturity for the ceremonies, principles, methods, and role clarity within Red Hat Operations’ SAFe op modelDirectly lead cross-team SAFe ceremoniesReview backlogs and planned work schedules to provide guidance on priority tasks for the AI roadmapMaintain the AI Jira instance and lead continued adoption and automation of the backlogs within and linked to itFoster a culture of continuous improvement via retrospectives, Inspect & Adapt sessions, and utilization of Jira reports and dataEnable repeatable planning through Sprint Planning, Refinement, and quarterly PI PlanningRaise potential issues or blockers with technical leads and provide mitigation options and needs to resolveEnsure all deliverables and commitments meet quality standards and expectationsMaintain discretion and confidentiality in all areas pertaining to data and proprietary information, whether internal to Red Hat or customer specific, as well as Personally Identifiable Information (PII) for Red Hat employees\n\nWhat Will You Bring\n\nDemonstrated understanding and expertise in Agile delivery methodologies. SAFe experience preferred5+ years of relevant project management experienceExperience participating in digital transformation projects where technology and organizational change go hand in hand to deliver long term sustainabilityFamiliarity with technology and data architecture techniques and deliveryKnowledge of and experience with Agile delivery and tracking tools, Jira strongly preferredExperience working with senior executives and system integratorsExpertise in creating presentation and informational collateral using a wide set of data and information of various sourcesBachelor's degree required; master's degree nice-to-haveAdept at adopting new technologies for day to day work activitiesStrong communications, stakeholder, and relationship management skillsAbility to synthesize and analyze complex data to identify potential risks and draw meaningful conclusions Must be comfortable operating in rapidly changing environmentsRTE and/or Scrum Master certification given strong consideration. PMP a bonus.\n\nAbout Red Hat\n\nRed Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver high-performing Linux, cloud, container, and Kubernetes technologies. Spread across 40+ countries, our associates work flexibly across work environments, from in-office, to office-flex, to fully remote, depending on the requirements of their role. Red Hatters are encouraged to bring their best ideas, no matter their title or tenure. We're a leader in open source because of our open and inclusive environment. We hire creative, passionate people ready to contribute their ideas, help solve complex problems, and make an impact.\n\nInclusion at Red Hat\n\nRed Hat’s culture is built on the open source principles of transparency, collaboration, and inclusion, where the best ideas can come from anywhere and anyone. When this is realized, it empowers people from different backgrounds, perspectives, and experiences to come together to share ideas, challenge the status quo, and drive innovation. Our aspiration is that everyone experiences this culture with equal opportunity and access, and that all voices are not only heard but also celebrated. We hope you will join our celebration, and we welcome and encourage applicants from all the beautiful dimensions that compose our global village.\n\nEqual Opportunity Policy (EEO)\n\nRed Hat is proud to be an equal opportunity workplace and an affirmative action employer. We review applications for employment without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, ancestry, citizenship, age, veteran status, genetic information, physical or mental disability, medical condition, marital status, or any other basis prohibited by law.\n\nRed Hat does not seek or accept unsolicited resumes or CVs from recruitment agencies. We are not responsible for, and will not pay, any fees, commissions, or any other payment related to unsolicited resumes or CVs except as required in a written contract between Red Hat and the recruitment agency or party requesting payment of a fee.\n\nRed Hat supports individuals with disabilities and provides reasonable accommodations to job applicants. If you need assistance completing our online job application, email application-assistance@redhat.com. General inquiries, such as those regarding the status of a job application, will not receive a reply.",
      "skills": [
        "Analytical Skills",
        "Jenkins",
        "Kubernetes",
        "Business Relationship Management",
        "Coaching",
        "Jira",
        "Linux",
        "Presentations",
        "Red Hat Linux",
        "Scrum"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266333304",
      "title": "Engineer II - Digital Engine Services ProActive Analyst",
      "company": "Pratt & Whitney",
      "location": "Bengaluru, Karnataka, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://careers.rtx.com/global/en/job/RAYTGLOBAL01779496EXTERNALENGLOBAL/Engineer-II-Digital-Engine-Services-ProActive-Analyst?utm_source=linkedin&utm_medium=phenom-feeds",
      "description": "Date Posted:\n\n2025-07-14\n\nCountry:\n\nIndia\n\nLocation:\n\nNorth Gate Business Park Sy.No 2/1, and Sy.No 2/2, KIAL Road, Venkatala Village, Chowdeshwari Layout, Yelahanka, Bangalore, Karnataka 560064\n\nPosition Role Type:\n\nUnspecified\n\nWhat Are Our Expectations\n\nWe are looking for a DES ProActive Junior Analyst who will be responsible to monitor engine performance and operability health trends for selected engines as a daily data analysis service. The candidate will be responsible for the review of engine anomalies, trend shifts and identify deterioration symptoms & signatures. The candidate will also be responsible to provide proactive recommendations and work in close cooperation with internal support teams to identify fleet availability and reliability drivers and opportunities for continuous improvement. The candidate will also be required to assist field service personnel in technical troubleshooting and AOG support as required and define new proactive engine health indicators.\n\nThe candidate needs to be a driven individual that can employ their industry experience with individual dynamism that can bring the front-line customer service team together to deliver comprehensive solutions. Excellent understanding of field and line maintenance activities, superior communication and people as well as information technology skills are fundamental. The candidate shall have extensive technical knowledge and experience with P&WC engine models and demonstrated understanding of engine performance health trending. The candidate shall also have knowledge of the ICA management.\n\nThe individual is recognized as highly self-motivated and a fast learner with strong communication, cross-functional teamwork, and problem-solving skills. The individual must have strong abilities to interface with customers, operators, aircraft maintenance personnel, engineers and OEMs to successfully complete projects and deliver world-class front-line services. Moreover, excellent analytical skills, proficiency in big data analysis methods and tools will also be contributors to the success of the individual selected. Finally, the candidate will be focused on lifelong learning and being recognized as an expert in their service support field.\n\nWhat your day to day will look like:\n\nMonitor engine performance and operability health trends for selected engines as a daily data analysis serviceReview engine anomalies, trend shifts and identify deterioration symptoms & signaturesReview issued alerts and provide visibility to P&WC Customer Support by way of a fleet based morning health trend status report using the DES internal web portalEvaluate engine FADEC fault codes, exceedances and other eventsConduct detailed mission and utilization analysis comparing power usage (actual vs. planned)Work in close cooperation with Reliability Engineer to identify fleet availability and reliability drivers and opportunities for continuous improvement by defining DES anticipation methods to reduce/eliminate such eventsReview oil analysis reports in conjunction with MOP and MOT trends to provide oil system health assessments where applicableWork closely with the DES analytics team to define new proactive engine health indicatorsPerform continuous improvement studies to maximize engine time on wing, understand the impact of various missions, environments, and other fleet characteristics to design customized recommendations for specific operators and initiate issuance of new SBs and FIM / EMM revisionsLog successes generated by the team and quantify business impact on the P&WC customer service organization\n\nThe tools you need to be successful:\n\nBachelors or Masters in Engineering, aerospace specialization or equivalentMinimum of 3 - 5 years’ of Customer Service engineering or relevant experience required in the field of aviationExperience with field and line maintenance activities and demonstrated understanding of engine performance health trending along with knowledge of the ICA management. Customer interface experience and front-line supportUnderstanding the aircraft engine data acquisition and current sensor technologyUnderstanding of engine performance calibration testing, trend monitoring practices and engine control systemsAbility to interpret engine Borescope Inspection reports and line/shop maintenance recordsReliability analysis practices and use of statistical data analysis toolsExperience with ICA documentation, drawings and specification interpretationKnowledge of PowerBI and Python an asset\n\nRTX adheres to the principles of equal employment. All qualified applications will be given careful consideration without regard to ethnicity, color, religion, gender, sexual orientation or identity, national origin, age, disability, protected veteran status or any other characteristic protected by law. \n\nPrivacy Policy and Terms:\n\nClick on this link to read the Policy and Terms",
      "skills": [
        "Analytical Skills",
        "Communication",
        "Engineering",
        "Problem Solving",
        "AOG",
        "Aerospace",
        "Engine Performance",
        "Service Engineering",
        "Troubleshooting",
        "Verilog"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264475604",
      "title": "Data Engineer",
      "company": "Steam-A",
      "location": "India",
      "workplace_type": "Remote",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/job-apply/4264475604",
      "description": "SENIOR DATA ENGINEER - Remote (India based)\nTo apply:Please share the below to gayathri@steam-a.com and preeti@steam-a.com\n1. Phone number2. Email id3. Total number of years of experience4. Number of years’ experience in Python programming language5. Number of years’ experience in AWS Services6. Number of years’ experience in Serverless Architecture7. Number of years’ experience in Infrastructure as Code (IaC)8. Number of years’ experience in Apache Airflow9. Number of years’ experience in Data Engineering10. Current CTC11. Expected CTC12. Are you an immediate joiner?13. Notice period/availability14. Current location15. Any leadership experience? Yes/No, # of years. # of members in the team?16. Updated CV\nKey Responsibilities:\nLead the design and architecture of scalable and reliable data engineering solutions on AWS, leveraging serverless architecture and best practices.Provide technical leadership and mentorship to data engineering team members, guiding them in designing and implementing robust data pipelines.Collaborate with cross-functional teams to understand mission requirements and translate them into scalable data solutions.Drive the adoption of best practices for data engineering, including code quality, performance optimization, and security.Design and implement efficient data models to support mission objectives, ensuring data integrity and performance.Manage and prioritize tasks for the data engineering team, ensuring timely delivery of highquality solutions.Lead code reviews and provide constructive feedback to team members to improve code quality and ensure adherence to best practices.Stay up-to-date with the latest technologies and trends in data engineering and recommend improvements and optimizations to existing systems.Work closely with DevOps team to integrate data pipeline deployments into CI/CD pipelines and automate infrastructure management tasks.\nQualifications:\nBachelor’s degree in Computer Science, Engineering, or related field.Significant experience in designing and implementing scalable data engineering solutions on AWS.Strong proficiency in Python programming language.Expertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.Experience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.Proven leadership skills with the ability to mentor and guide junior team members.Excellent understanding of data modelling concepts and experience with tools like ERStudio.Strong communication and collaboration skills, with the ability to work effectively in a crossfunctional team environment.Experience with Apache Airflow for orchestrating data pipelines is a plus.Knowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nThank you.",
      "skills": [
        "Amazon Web Services (AWS)",
        "Computer Science",
        "Data Analytics",
        "Data Engineering",
        "Programming Languages",
        "Python (Programming Language)",
        "Apache Airflow",
        "Data Pipelines",
        "Extract, Transform, Load (ETL)",
        "Infrastructure as code (IaC)"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266330826",
      "title": "Sr Software Engineer",
      "company": "HERE Technologies",
      "location": "Mumbai, Maharashtra, India",
      "workplace_type": "Hybrid",
      "experience_level": "",
      "job_url": "https://careers-here.icims.com/jobs/78133/sr-software-engineer/job?mode=apply&iis=LinkedIn",
      "description": "What's the role? \n\nWe are looking for experienced candidates who will be able to work effectively in a multidisciplinary team environment focused on innovation and be able to learn and integrate complementary technologies inside and outside their specific area of expertise to deliver highly scalable and fault tolerant systems to optimize the way content feeds in to all of our products in the automotive, enterprise, and consumer spaces.With a global fleet of vehicles collecting and sending petabytes of sensor, street-level and aerial data every year, and rich set of data coming from various other sources,\n\n Be a part of the engineering team charged with designing, architecting and building a platform to transform these massive data sets into building blocks for content creation.  Work with data science in order to rapidly integrating machine learned models into production systems  Building out services and APIs providing direct access to these rich datasets.  Work with Amazon Web Services as a storage and computing platform.  Own the framework that is leveraged across the organization for accelerating time to market and insuring compliance with standards that promote availability, scalability and security.  Technical architectural design and review.  Software implementation with fully automated testing.  Peer code reviews, mentoring and knowledge sharing sessions.  Continuous process improvement, coding standards, use of code quality metrics.  Build, release and deployment automation and monitoring. \n\n Who are you? \n\n 4+ years of software design and development experience with distributed highly scalable systems, with at least 2+ years in Lead Engineering role.  Must have hands-on development in object oriented languages as well as functional programming skills with Java, Scala, Python  Experience building REST web services  Develop and deploy highly scalable and efficient solutions for data analytics in AWS  Build out systems to monitor deployed workflows and alert/handle failures  Clearly document the architecture and design  Effective integration, contract and unit testing  Experience with build, Continuous Integration & Deployment, and unit-testing technologies including JUnit.  Familiar with structured code reviews, e.g Gerrit, Crucible and automated deployment with Docker. \n\nHERE is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, age, gender identity, sexual orientation, marital status, parental status, religion, sex, national origin, disability, veteran status, and other legally protected characteristics.\n\n Who are we? \n\nHERE Technologies is a location data and technology platform company. We empower our customers to achieve better outcomes – from helping a city manage its infrastructure or a business optimize its assets to guiding drivers to their destination safely.\n\nAt HERE we take it upon ourselves to be the change we wish to see. We create solutions that fuel innovation, provide opportunity and foster inclusion to improve people’s lives. If you are inspired by an open world and driven to create positive change, join us. Learn more about us on our YouTube Channel.\n\nIn this role you be part of the Content Automation group. You will work alongside high energy, innovative teams leveraging cutting edge technology to identify new technologies, processes, and sources to build the best-in-class high definition map used in autonomous driving. You will be enabling one of the largest technological and cultural shifts in the modern world, creating enhanced road safety, reduced congestion, lower emissions, and expanded mobility, paving the way for a more efficient and sustainable transportation future.",
      "skills": [
        "Amazon Web Services (AWS)",
        "Java",
        "Jenkins",
        "Kubernetes",
        "Microservices"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264475859",
      "title": "Lead Data Scientist",
      "company": "Arivonix",
      "location": "Greater Coimbatore Area",
      "workplace_type": "Remote",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/job-apply/4264475859",
      "description": "Master’s degree in Data Science, Mathematics, or a related field.5+ years of experience in Data Science, Machine Learning, or a similar domain.Expertise in linear algebra, statistics, and probability, with hands-on experience in statistical testing, regression, and deep learning techniques.Proficiency in machine learning frameworks such as TensorFlow, PyTorch, or MxNet.Experience with video processing, video codecs, and video management systems.Strong understanding of design patterns, algorithms, and data structures to deliver efficient solutions.Advanced development experience in Python and libraries like NumPy, Sci-Kit Learn, and Matplotlib.Proven track record of model performance tuning and deploying optimized machine learning modelsPreferred Skills:Proficiency in creating scalable machine learning pipelines.Familiarity with industry standards for code optimization and debugging. Education Requirements:Master’s degree in Data Science, Mathematics, or a related discipline is mandatory.\n Key Skills:Data Science, Machine Learning, TensorFlow, PyTorch, Deep LearningExpertise in Python, backend frameworks (Flask, FastAPI), and cloud platforms (AWS/Azure/GCP).\n",
      "skills": [
        "Artificial Intelligence (AI)",
        "Data Mining",
        "Data Science",
        "Deep Learning",
        "Machine Learning",
        "Mathematics",
        "Natural Language Processing (NLP)",
        "Pattern Recognition",
        "Performance Tuning",
        "Statistics"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266545086",
      "title": "Full Stack Flutter Developer",
      "company": "Workerlly Tech Private Limited",
      "location": "Panchkula, Haryana, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266545086",
      "description": "Job Title: Flutter Developer (Cross-Platform + AI/ML Integration + WebSocket + Front-End Design)Location: Panchkula \nJob Type: Full-TimeExperience: Minimum 4 Years (In similar development environments and startup ecosystem)\nJob Summary:\nWe are hiring a Flutter Developer with strong hands-on experience in cross-platform development, AI/ML integration, WebSocket communication, and front-end UI/UX design. The ideal candidate must be highly skilled in Flutter, Python, React, Docker, and REST APIs, and must have previously worked in high-paced startup environments.\nKey Responsibilities:\n1. Develop and maintain high-quality cross-platform mobile apps using Flutter.2. Collaborate with AI/ML engineers to integrate ML models into mobile applications.3. Implement and manage WebSocket-based real-time communication features.4. Design intuitive front-end interfaces with a focus on responsive UI/UX.5. Build and consume RESTful APIs and integrate with backend services.6. Use GitHub for version control, pull requests, and collaboration.7. Deploy and manage services using Docker (basic to intermediate level).8. Optimize application performance, manage security, and troubleshoot bugs.9. Work closely with product managers and designers in a startup-style agile environment.\nRequired Skills:\n> 4+ years of professional experience in mobile development (Flutter).> Proficiency in Flutter/Dart, with a strong understanding of cross-platform architecture.> Strong command of Python, especially for AI/ML integration (TensorFlow, PyTorch, etc.).> Proven experience working with WebSockets and real-time communication protocols.> Solid front-end design capabilities — UI/UX implementation using Flutter or React.> Good knowledge of ReactJS (for web components or admin panels).> Experience with Docker, containers, and basic DevOps practices.> Expertise in API integration, JSON handling, and secure data exchange.> Strong experience with GitHub, including branching, code reviews, and CI/CD workflows.> Startup mindset: proactive, adaptive, and problem-solving attitude.\nBonus Skills (Preferred but Not Mandatory):\n* Experience with Firebase, push notifications, analytics tools.* Familiarity with cloud services (AWS, GCP, or Azure).* Understanding of app deployment on Play Store and App Store.* Knowledge of state management tools (Bloc, Provider, Riverpod, etc.).* Experience with writing test cases (unit, widget, and integration tests).\nWhat We Offer:\n~ A high-impact role in a fast-growing tech startup.~ Opportunity to work on real-world AI/ML powered solutions.~ Work with a passionate, driven, and collaborative team.~ Flexible working hours and growth-oriented environment. \nInterseted please share your resume on hr@workerlly.com",
      "skills": [
        "Application Programming Interfaces (API)",
        "Docker Products",
        "Front-End Development",
        "Machine Learning",
        "Python (Programming Language)",
        "REST APIs",
        "React.js",
        "Representational State Transfer (REST)",
        "Software Development",
        "Mobile Applications"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264481078",
      "title": "Intermediate Applications Developer - Java, GCP, Angular",
      "company": "UPS",
      "location": "Chennai, Tamil Nadu, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://www.jobs-ups.com/imea/fr/job/UPBUPSGLOBALR25023690EXTERNALFRIMEA/Intermediate-Applications-Developer-Java-GCP-Angular?utm_source=linkedin&utm_medium=phenom-feeds",
      "description": "Avant de postuler à un emploi, sélectionnez votre langue de préférence parmi les options disponibles en haut à droite de cette page.\n\nDécouvrez votre prochaine opportunité au sein d'une organisation qui compte parmi les 500 plus importantes entreprises mondiales. Envisagez des opportunités innovantes, découvrez notre culture enrichissante et travaillez avec des équipes talentueuses qui vous poussent à vous développer chaque jour. Nous savons ce qu’il faut faire pour diriger UPS vers l'avenir : des personnes passionnées dotées d’une combinaison unique de compétences. Si vous avez les qualités, de la motivation, de l'autonomie ou le leadership pour diriger des équipes, il existe des postes adaptés à vos aspirations et à vos compétences d'aujourd'hui et de demain.\n\nJob Description\n\nFiche de poste :\n\nMoving our world forward by delivering what matters! UPS is a company with a proud past and an even brighter future. Our values define us. Our culture differentiates us. Our strategy drives us. At UPS we are customer first, people led and innovation driven. UPS’s India based Technology Development Centers will bring UPS one step closer to creating a global technology workforce that will help accelerate our digital journey and help us engineer technology solutions that drastically improve our competitive advantage in the field of Logistics.\n\n‘Future You’ grows as a visible and valued Technology professional with UPS, driving us towards an exciting tomorrow. As a global Technology organization we can put serious resources behind your development. If you are solutions orientated, UPS Technology is the place for you. ‘Future You’ delivers ground-breaking solutions to some of the biggest logistics challenges around the globe. You’ll take technology to unimaginable places and really make a difference for UPS and our customers.\n\nResponsibilities\n\n Perform systems analysis and design. Design and develop moderate to complex applications. Develops and ensures creation of application documents. Defines and produces integration builds. Monitors emerging technology trends.\n\nPrimary Skills\n\n Mastery of Core Java concepts and Java EE. Good experience with Spring and Spring Boot. Good experience with CI/CD code automation and DevOps Good experience with GCP Good experience with OpenShift Good experience with Angular Strong understanding of software architecture and design principles. Ability to design scalable, maintainable, and efficient Java & .Net applications. Understanding of data structures, programming logic, and design Understanding of application design patterns Excellent written & verbal communication skills Excellent attention to detail\n\nSecondary Skills\n\n Experience using ArgoCD Experience with Structured Query Language (SQL) Experience using OAuth/OIDC\n\nQualifications\n\n 5+ years of experience Bachelor’s Degree or International equivalent\n\nType De Contrat\n\nen CDI\n\nChez UPS, égalité des chances, traitement équitable et environnement de travail inclusif sont des valeurs clefs auxquelles nous sommes attachés.",
      "skills": [
        "Angular",
        "Hibernate",
        "Jakarta EE",
        "Java",
        "Spring Boot",
        "Spring Framework",
        "Attention to Detail",
        "Google Cloud Platform (GCP)",
        "Graphic Design Principles",
        "OpenShift"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264483253",
      "title": "Exponentia.ai - Senior Engineer - Generative AI Solutions",
      "company": "Exponentia.ai",
      "location": "Mumbai Metropolitan Region",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264483253",
      "description": "Job Responsibilities\n\n Lead the development and deployment of Generative AI solutions using Databricks and advanced web frameworks. Architect and implement APIs to serve Generative AI models for production environments. Design end-to-end solutions for clients, focusing on GenAI applications, ensuring scalability and performance optimization. Provide expertise on pricing models, manage cost estimates, and develop Bill of Materials (BoM) for client projects. Collaborate with clients and internal teams to understand requirements and deliver tailored GenAI solutions. Document workflows, API structures, and architectural designs, ensuring transparency and clarity for all stakeholders. Effectively communicate with clients, providing regular updates on project progress and addressing any technical Skills : Minimum 7 years of relevant experience in AI/ML development and deployment. Proven experience developing and deploying end-to-end applications in production environments. Extensive hands-on experience with Databricks for building data pipelines and GenAI model development. Strong expertise in building and deploying APIs using modern web frameworks to serve GenAI models. In-depth knowledge of Generative AI models (e.g., GPT, LLAMA, Gemini, Anthropic and Open source models) and API integration. Experience conducting architecture and technical discussions with clients. Proficient in Python and libraries for GenAI (e.g., Hugging Face, OpenAI, Open source or custom-built transformer models). Experience with cloud platforms (preferably Azure/AWS) for deploying AI and GenAI solutions. Strong architectural skills for designing scalable and performant GenAI applications. Implement and integrate advanced LLM (Large Language Models) techniques, including LangChain, LlamaIndex, and other frameworks, for developing GenAI solutions. Apply prompt engineering techniques to optimize GenAI model outputs for specific client use cases. Ensure robust AI safety guardrails are in place during the development of Generative AI applications, focusing on prevention of unsafe outputs.\n\n(ref:hirist.tech)",
      "skills": [
        "Amazon Web Services (AWS)",
        "Artificial Intelligence (AI)",
        "Computer Science",
        "Computer Vision",
        "Data Science",
        "Deep Learning",
        "Large Language Models (LLM)",
        "Machine Learning",
        "Model Development",
        "Statistics"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264475919",
      "title": "Java Software Engineer",
      "company": "Oracle",
      "location": "Pune, Maharashtra, India",
      "workplace_type": "Hybrid",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/job-apply/4264475919",
      "description": "NOTE - Saturday 19th July interview will be in person for Pune/Mumbai Location. Kindly apply accordingly.\nJob Summary We are seeking an experienced professional to join our Banking Domain and Practice team as a Senior Technical Consultant. The successful candidate will support senior consultants, project managers, and teams of business and technology consultants in delivering business-focused solutions for our clients using Oracle applications, tools, and technology.Key ResponsibilitiesParticipate as a senior technical team member in development, implementation, or managed service activity streams, such as migration, integration, testing, and coordination with other Oracle teamsAssist with analysis, design, and implementation of solutions for various business requirementsDevelop programs according to specifications and requirements, meeting planned schedules and quality expectationsDocument all work in accordance with agreed standards and processes, adhering to change control and audit/compliance requirementsEnsure quality code delivery, perform proper handovers, and provide timely status reports to supervisorsCollaborate with clients, presenting solutions and providing excellent customer interaction skillsStay up-to-date with industry trends, technologies, and banking terminologies, and apply this knowledge to deliver high-quality solutionsQualifications & Skills Mandatory:5-12 years of experience in software development, with at least 5 years in primary skill areas (Core Java, J2EE, Microservices, Spring Boot, JavaScript, XML, Oracle SQL, PL/SQL, and Oracle Database)Exposure to software deployment and troubleshooting on Application Server software, especially Oracle WebLogicWorking knowledge of release management and source control toolsGood client interaction skills, presentation skills, and awareness of banking terminologies and conceptsStrong communication and documentation skills in EnglishBachelor's degree in Computer Science or equivalentGood-to-Have:Exposure to Banking Domain, software development processes, and practices (DevOps tools, Testing tools)Experience in development, implementation, and/or support of core banking applicationsOracle Banking/FLEXCUBE Technical certifications in functional areas",
      "skills": [
        "Java",
        "PL/SQL",
        "Spring Boot",
        "Core Banking",
        "WebLogic"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264479722",
      "title": "Senior ML Engineer",
      "company": "eBay",
      "location": "Bengaluru, Karnataka, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://jobs.ebayinc.com/us/en/job/EBAEBAUSR0068404EXTERNALENUS/Senior-ML-Engineer?utm_source=linkedin&utm_medium=phenom-feeds",
      "description": "At eBay, we're more than a global ecommerce leader — we’re changing the way the world shops and sells. Our platform empowers millions of buyers and sellers in more than 190 markets around the world. We’re committed to pushing boundaries and leaving our mark as we reinvent the future of ecommerce for enthusiasts.\n\nOur customers are our compass, authenticity thrives, bold ideas are welcome, and everyone can bring their unique selves to work — every day. We're in this together, sustaining the future of our customers, our company, and our planet.\n\nJoin a team of passionate thinkers, innovators, and dreamers — and help us connect people and build communities to create economic opportunity for all.\n\nAbout The Team And The Role\n\nAs an ML Engineer at eBay's Risk team, you'll be part of an exciting journey that is instrumental in maintaining eBay to be a safe and trusted marketplace. The role involves working on critical projects related to the Risk product, which enhances the model lifecycle management for applied researchers, data scientist and ML engineers like yourself. This team is known for fostering an innovative culture and a collaborative environment, encouraging continuous learning and growth. The Risk Engineering team is a key player in keeping the marketplace safe for all buyers and seller.\n\nWhat You Will Accomplish\n\nDrive innovations in building models using ML/NLP/AI to prevent fraud and increase trust at eBay marketplace.Collaborate across multifunctional teams to integrate cutting-edge ML solutions that directly impact eBay's Risk mitigation and fraud prevention.Implement and refine model deployment processes that facilitate quick and efficient model releases from offline to online environments.Contribute to multi-functional problem-solving activities, assisting in developing advanced technological solutions for complex business situations.Engage in continuous learning opportunities, staying updated with the latest industry trends and technologies to drive eBay's success further.\n\n\nWhat You Will Bring\n\nSolid experience in developing ML/NLP solutions with at least 4+ years in the industry.Proven skills in enhancing model lifecycle management, particularly within complex ML platforms.Ability to work effectively in a collaborative and agile environment, managing multiple projects simultaneously.Deep understanding of both CPU and GPU serving platforms, and effectively bridging technological gaps.Experience working in large-scale AI transformations within prominent product companies in a plus.Strong communication skills and the ability to articulate complex technical concepts to diverse audiences.\n\n\nPlease see the Talent Privacy Notice for information regarding how eBay handles your personal data collected when you use the eBay Careers website or apply for a job with eBay.\n\neBay is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you have a need that requires accommodation, please contact us at talent@ebay.com. We will make every effort to respond to your request for accommodation as soon as possible. View our accessibility statement to learn more about eBay's commitment to ensuring digital accessibility for people with disabilities.\n\nThe eBay Jobs website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies. Visit our Privacy Center for more information.",
      "skills": [
        "Algorithms",
        "Amazon Web Services (AWS)",
        "Computer Vision",
        "Data Science",
        "Deep Learning",
        "Machine Learning",
        "Natural Language Processing (NLP)",
        "Problem Solving",
        "Python (Programming Language)",
        "Product Lifecycle Management"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266332810",
      "title": "Applications Engineering Internship",
      "company": "Synopsys Inc",
      "location": "Bengaluru, Karnataka, India",
      "workplace_type": "",
      "experience_level": "",
      "job_url": "https://careers.synopsys.com/job/-/-/44408/76799026480?source=LinkedIn",
      "description": "We Are: \"Drive technology innovations that shape the way we live and connect. Our technology drives the Era of Pervasive Intelligence, where smart tech and AI are seamlessly woven into daily life. From self-driving cars and health-monitoring smartwatches to renewable energy systems that efficiently distribute clean power, Synopsys creates high-performance silicon chips that help build a healthier, safer, and more sustainable world.\"\n\nInternship Experience:\n\n\"At Synopsys, interns dive into real-world projects, gaining hands-on experience while collaborating with our passionate teams worldwide—and having fun in the process! You'll have the freedom to share your ideas, unleash your creativity, and explore your interests. This is your opportunity to bring your solutions to life and work with cutting-edge technology that shapes not only the future of innovation but also your own career path. Join us and start shaping your future today!\"\n\nMission Statement:\n\n\"Our mission is to fuel today’s innovations and spark tomorrow’s creativity. Together, we embrace a growth mindset, empower one another, and collaborate to achieve our shared goals. Every day, we live by our values of Integrity, Excellence, Leadership, and Passion, fostering an inclusive culture where everyone can thrive—both at work and beyond.\"\n\nWhat You’ll Be Doing:\n\nDevelop knowledge on protocols and products based on industry leading protocols such as I3C, DDR, SD, AXI, AHBGain knowledge on ASIC flow and learn various tools used in the ASIC flowWork with a team of highly experienced applications engineers to provide comprehensive consulting on our IP productsParticipate in providing feedback on product documentation and collateralDevelop presentation and team working skills\n\nWhat You’ll Need:\n\nCurrently pursuing a Master’s degree in Electrical/Electronic/Computer Engineering with exposure to VLSI, Verilog, coding skills, FPGAsA self-motivated and excellent communicator will be ideal candidate for this role\n\nEqual Opportunity Statement:\n\n\"Synopsys is committed to creating an inclusive workplace and is an equal opportunity employer. We welcome all qualified applicants to apply, regardless of age, color, family or medical leave, gender identity or expression, marital status, disability, race and ethnicity, religion, sexual orientation, or any other characteristic protected by local laws. If you need assistance or a reasonable accommodation during the application process, please reach out to us.\"",
      "skills": [
        "Communication",
        "Computer Engineering",
        "Databases",
        "Application-Specific Integrated Circuits (ASIC)",
        "DDR SDRAM",
        "Presentations",
        "Technical Support",
        "Troubleshooting",
        "Verilog",
        "Very-Large-Scale Integration (VLSI)"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264482341",
      "title": "Software Engineer, II",
      "company": "Zebra Technologies",
      "location": "Bengaluru, Karnataka, India",
      "workplace_type": "",
      "experience_level": "",
      "job_url": "https://zebra.eightfold.ai/careers?pid=343627535801&domain=zebra.com&src=srm_linkedin_jb",
      "description": "Remote Work: Hybrid\n\nOverview:\n\nAt Zebra, we are a community of innovators who come together to create new ways of working to make everyday life better. United by curiosity and care, we develop dynamic solutions that anticipate our customer’s and partner’s needs and solve their challenges.\n\nBeing a part of Zebra Nation means being seen, heard, valued, and respected. Drawing from our diverse perspectives, we collaborate to deliver on our purpose. Here you are a part of a team pushing boundaries to redefine the work of tomorrow for organizations, their employees, and those they serve.\n\nYou have opportunities to learn and lead at a forward-thinking company, defining your path to a fulfilling career while channeling your skills toward causes that you care about – locally and globally. We’ve only begun reimaging the future – for our people, our customers, and the world.\n\nLet’s create tomorrow together.\n\nAnalyzes, develops, designs, and maintains software for the organization's products and systems. Performs system integration of software and hardware to maintain throughput and program consistency. Develops, validates, and tests: structures and user documentation. Work may be reviewed for accuracy and overall adequacy. Follows established processes and directions.\n\nWe are seeking a passionate and detail-oriented SDET to join our QA engineering team. The ideal candidate will have strong Python programming skills and hands-on experience testing cloud-native microservices deployed on Google Cloud Platform (GCP) or equivalant Cloud platform. You will be responsible for designing and implementing automated test frameworks, ensuring the reliability and scalability of distributed systems.\n\nResponsibilities:\n\nSDET – Python & GCP Microservices Testing\n\nExperience Level: 2–4 Years \n\nKey ResponsibilitiesDevelop and maintain automated test suites using Python and frameworks like Pytest, Robot FrameworkDesign and execute test cases for RESTful APIs, microservices, and event-driven architecturesCollaborate with developers and DevOps to integrate tests into CI/CD pipelines (e.g., Jenkins, GitHub Actions)Perform functional, integration, regression, and performance testingValidate deployments and configurations in GCP environments using tools like Cloud Build, GKE, and TerraformMonitor and troubleshoot test failures, log issues, and ensure timely resolutionContribute to test strategy, documentation, and quality metrics Implement AI-driven testing methodologies to enhance test coverage and efficiency. \nRequired Skills\n\n2–4 years of experience in software testing or SDET rolesStrong proficiency in Python for test automationExperience testing microservices and cloud-native applicationsFamiliarity with GCP services such as Cloud Functions, Pub/Sub, Cloud Run, and GKEHands-on experience with Docker, Kubernetes, and Linux-based environmentsKnowledge of Git, Jenkins, and CI/CD workflowsUnderstanding of QA methodologies, SDLC, and Agile practices\n\nPreferred Skills\n\nExposure to performance testing tools like JMeter, Locust, or k6Experience with JenkinsFamiliarity with Github Co-Pilot MongoDBKnowledge of BDD/TDD practices\n\nQualifications:\n\nSDET – Python & GCP Microservices Testing\n\nExperience Level: 2–4 Years\n\nRequired Skills2–4 years of experience in software testing or SDET rolesStrong proficiency in Python for test automationExperience testing microservices and cloud-native applicationsFamiliarity with GCP services such as Cloud Functions, Pub/Sub, Cloud Run, and GKEHands-on experience with Docker, Kubernetes, and Linux-based environmentsKnowledge of Git, Jenkins, and CI/CD workflowsUnderstanding of QA methodologies, SDLC, and Agile practices\nPreferred Skills\n\nExposure to performance testing tools like JMeter, Locust, or k6Experience with JenkinsFamiliarity with Github Co-Pilot MongoDBKnowledge of BDD/TDD practices Nice-to-Have Experience with LLMs and generative AI platforms. Contributions to open-source testing frameworks or AI communities. \n\nPreferred Education: Bachelor's or Masters degree in an appropriate engineering discipline required.\n\nPreferred Work Experience (years):Bachelors degree and 2+ years or Masters degree with no experience.All Other Regions: Bachelor’s or Master’s degree in an appropriate engineering discipline requiredPreferred work experience (years): 2+ years experience\n\nTo protect candidates from falling victim to online fraudulent activity involving fake job postings and employment offers, please be aware our recruiters will always connect with you via @zebra.com email accounts. Applications are only accepted through our applicant tracking system and only accept personal identifying information through that system. Our Talent Acquisition team will not ask for you to provide personal identifying information via e-mail or outside of the system. If you are a victim of identity theft contact your local police department.",
      "skills": [
        "Git",
        "Microservices",
        "Programming",
        "Python (Programming Language)",
        "REST APIs",
        "Software Development",
        "Software Testing",
        "Test Automation",
        "Google Cloud Platform (GCP)",
        "Linux"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264478093",
      "title": "Back End Developer",
      "company": "CodeVyasa ",
      "location": "Gurugram, Haryana, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/job-apply/4264478093",
      "description": "Looking for Backend Developer | Gurgaon to join a team of rockstar developers. The candidate should have a minimum of 5 yrs. of experience.\nThere are multiple openings. If you're looking for career growth & a chance to work with the top 0.1% of developers in the industry, this one is for you! You will report into IIT'ans/BITS grads with 10+ years of development experience + work with F500 companies (our customers).\nCompany Background - We are a multinational software company that is growing at a fast pace. We have offices in Florida & New Delhi. Our clientele spreads across the US, Australia & APAC. Here's the link to our website (codevyasa.com). To give you a sense of our growth rate, we've added 70+ employees in the last 6 weeks itself and expect another 125+ by the end of Q1 2025.\nKey Responsibilities:Design, develop, test, and maintain backend services using Python (Django/Flask/FastAPI) and Node.js (Express/NestJS).Build and maintain RESTful and/or GraphQL APIs.Integrate and manage relational and non-relational databases (PostgreSQL, MongoDB).Collaborate with front-end developers, DevOps, and product teams to build and deliver features end-to-end.Write clean, modular, and reusable code with proper documentation and test coverage.Optimize application performance and scalability.Participate in code reviews, design discussions, and agile development processes.Required Skills:Strong proficiency in Python and one or more frameworks: Django, Flask, or FastAPI.Solid experience with Node.js, especially using Express.js or NestJS.Expertise in PostgreSQL and MongoDB, including schema design and optimization.Experience building RESTful APIs; GraphQL knowledge is a plus.Familiarity with version control tools (e.g., Git) and CI/CD pipelines.Good understanding of backend architecture, microservices, and containerization concepts (Docker is a plus).Ability to write unit and integration tests.\nHere is what we have on offer for youGlassdoor's rating of 4.8Free healthcareLaser-sharp focus on upskilling our employeesDiverse & Inclusive teamsIndustry-par compensation & benefitsGreat work-life balance",
      "skills": [
        "Django",
        "MongoDB",
        "Node.js",
        "PostgreSQL",
        "Python (Programming Language)",
        "NestJS"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266329738",
      "title": "PERSEUS Integration Engineer",
      "company": "BNP Paribas",
      "location": "Mumbai, Maharashtra, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://group.bnpparibas/en/careers/job-offer/perseus-integration-engineer?SRC=LINKEDIN",
      "description": "ITG FRESH – Performance Management & Liquidity Specialized Solutions\n\nAbout BNP Paribas Group\n\nBNP Paribas is a top-ranking bank in Europe with an international profile. It operates in 71 countries and has almost 199 000 employees. The Group ranks highly in its three core areas of activity: Domestic Markets and International Financial Services (whose retail banking networks and financial services are grouped together under Retail Banking & Services) and Corporate & Institutional Banking, centred on corporate and institutional clients. The Group helps all of its clients (retail, associations, businesses, SMEs, large corporates and institutional) to implement their projects by providing them with services in financing, investment, savings and protection. In its Corporate & Institutional Banking and International Financial Services activities, BNP Paribas enjoys leading positions in Europe, a strong presence in the Americas and has a solid and fast-growing network in the Asia/Pacific region.\n\nAbout BNP Paribas India Solutions\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, a leading bank in Europe with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 6000 employees, to provide support and develop best-in-class solutions.\n\nAbout Businessline/Function\n\nFRESH – Performance Management and Liquidity Solutions (PMLS)\n\nFRESH has been created in January 2020, as a joint venture of Finance functional project teams with their counterpart ITg IT project teams. FRESH is hierarchically part of ITg, but with a very strong functional link with Finance, its sponsoring department. The main purpose of its creation is to align the efforts of Finance and ITg and accomplish the delivery of projects in the most timely and cost effective way.\n\nPMLS is one of the main departments in FRESH covering Liquidity and Performance Management streams.\n\nJob Title\n\nPerseus Application Integrator\n\nDate\n\n30-May-2025\n\nDepartment\n\nITG - FRESH\n\nLocation:\n\nChennai/Thane\n\nBusiness Line / Function\n\nPMLS\n\nReports To\n\n(Direct)\n\nGrade\n\n(if applicable)\n\n(Functional)\n\nNumber Of Direct Reports\n\nDirectorship / Registration:\n\nNA\n\nPosition Purpose\n\n Purpose: Responsible for the Maintenance and configuration of the PERSEUS application which is a BNP Paribas group Performance Management application used to store and generate all group wide financial reporting Scope: ISPL based PMLS IT team. Responsibility for the IT maintenance and enhancement of PMLS applications\n\nResponsibilities\n\nPerform the analysis around the application, writing technical specifications (use case/ requirement) on new needs and requirements\n\nProvide Operational and technical support on incident analysis and monitoring.\n\n Infrastructure management and monitoring. Installation and configuration of applications and solutions. Coordination of environment delivery Development of technical solutions Proposal of scenarios for implementation and integration of solutions, Monitoring of customer technical assistance requests Evaluation of the quality and application and technical performance of the solutions implemented, Definition of technical clauses, functional specifications and drafting of specifications, Proposal of scenarios for functional evolutions of the architectures and systems in place in a logic of standardization and homogenization of the IS, Identification and analysis of malfunctions and propose solutions and developments, Writing installation and operating records Execution of the acceptance and monitoring of corrections of anomalies Participate in an Application Level 3 Support Qualify and resolve incidents, contributes to the resolution of problems. Participate in acceptance testing for procedures produced\n\nAreas of intervention can include the following: Databases, Operating, security and middleware products, Storage/Archiving, Operating systems and Telecoms\n\nTechnical & Behavioral Competencies\n\nEssential\n\n Knowledge of N-tier architecture: Presentation/Application layer, Business layer and Data access layer. Web application installation knowledge Industrial installation experience with Ansible Unix/Linux Shell Scripting Management of Obsolescence. Experience working on WebSphere Application Server User Support A positive and collaborative attitude. Strong analysis and problem-solving skills Strong communication skills.\n\nPreferred\n\n Knowledge of Performance Management Interest in finance topics Knowledge of DevOps\n\nSpecific Qualifications (if Required)\n\n BE/BTech Degree\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nAbility to collaborate / Teamwork\n\nAttention to detail / rigor\n\nCommunication skills - oral & written\n\nClient focused\n\nTransversal Skills: (Please select up to 5 skills)\n\nAbility To Develop Others & Improve Their Skills\n\nAnalytical Ability\n\nAbility to inspire others & generate people's commitment\n\nChoose an item.\n\nChoose an item.\n\nEducation Level\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 2 years",
      "skills": [
        "Analytical Skills",
        "Problem Solving",
        "Software Development",
        "Attention to Detail",
        "Linux",
        "Presentations",
        "Scripting",
        "Telecommunications",
        "Troubleshooting",
        "WebSphere"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264483449",
      "title": "Fullstack React Developer",
      "company": "Elliot Systems",
      "location": "Pune, Maharashtra, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4264483449",
      "description": "\nWhat You’ll Do\n\nDevelop end-to-end features using React 18 + TypeScript with Tailwind CSS and shadcn/ui\nBuild scalable and performant UIs using Vite, Radix UI, and class-variance-authority\nArchitect and implement backend logic using Supabase, PostgreSQL, and serverless edge functions\nIntegrate with AI-powered microservices such as document analyzers, multi-agent orchestrators, and compliance engines\nBuild and maintain robust state management flows using TanStack Query, Zustand, and React Hook Form\nOwn routing and navigation flows with React Router DOM and path aliasing\nCollaborate on testing with Playwright, Vitest, and Testing Library\nImplement and optimize security features: JWT Auth, RLS, input sanitization, and CORS configuration\nWork on real-time, multi-tenant data systems with row-level security and real-time trigger\n\n\nWhat We’re Looking For\n\n8+ years of hands-on experience as a Fullstack or Frontend-focused Developer\nExpertise in React + TypeScript and modern frontend tooling\nStrong knowledge of state management, form handling, component design\nExperience working with Supabase, PostgreSQL, or similar BaaS/FaaS platforms\nProficiency in building and consuming serverless APIs\nFamiliarity with AI integration, document processing, or multi-agent systems is a bonus\nSolid understanding of testing practices and performance optimizations\nPassion for clean code, type safety, DX, and modern architecture principles\n\nContact me at 9823132466 and share CV over WhatsApp.\n\nThanks,\nAdvait Samant",
      "skills": [
        "Application Programming Interfaces (API)",
        "Back-End Web Development",
        "Cascading Style Sheets (CSS)",
        "Front-End Development",
        "JSON",
        "React.js",
        "Redux.js",
        "TypeScript",
        "HTML5",
        "Testing Practices"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266333310",
      "title": "Senior Engineer - Data Science",
      "company": "Marsh",
      "location": "Mumbai, Maharashtra, India",
      "workplace_type": "Hybrid",
      "experience_level": "",
      "job_url": "https://careers.marshmclennan.com/global/en/job/R_312699/Senior-Engineer-Data-Science?utm_source=linkedin&utm_medium=phenom-feeds&source=LinkedIn_Slots",
      "description": "Marsh McLennan Global Services (MMGS) India is seeking candidates for the following position based in the Mumbai office:\n\nJob Profile: Senior Engineer - Data Science\n\nMMC Business Unit: Marsh\n\nMMGS Function: Knowledge Services\n\nMarsh is a global leader in insurance broking and risk management. In more than 130 countries, our experts in every facet of risk and across industries help clients to anticipate, quantify, and more fully understand the range of risks they face.\n\nMarsh Advisory is the consultative branch of Marsh, provides solutions in the increasing needs of our clients to implement risk management programs within their organization. Marsh Advisory helps companies to change their risk profiles so they can improve resiliency, reduce claims, and minimize the total cost of risk. Businesses today regularly tackle multiple challenges, whether facing property and casualty, cyber, pandemic, ERM / BCP / BCM, climate change, supply chain, reputational, or other risks, Marsh Advisory can help.\n\nMMGS is a global knowledge center for Marsh McLennan and houses teams, which work closely with the colleagues across various operating units and locations. The Knowledge Services function under MMGS aims to provide specialized services in the domain of Research, Consulting, Data Analytics, Data Science, Actuarial and Design.\n\nThis role at Knowledge Services aims to provide colleagues with an exposure towards key risks faced by businesses. Incumbent will also have an opportunity to contribute to cutting edge analytics platform and products. Incumbent is expected to be an expert of Data Science algorithms, Business Analytics and Data Manipulation. Expertise in Python and SQL is a must. Experience with Insurance or Banking industry is a plus.\n\nPosition Details\n\nThis position is for an individual contributor in Data Science, who will develop and implement models using leading-edge techniques in machine learning, predictive modeling, artificial intelligence, and natural language processing as applied in commercial insurance and risk management. This position consults with clients and colleagues on complex financial and statistical analyses, and develops approaches for new, market-leading analytics-based tools.\n\nResponsibilities\n\nDevelops modeling approaches for implementing new, market-leading analytics-based tools to understand and address riskUnderstands business problems to create an approach that starts with determining structured and unstructured data needs and availability, builds Statistical Models, Machine Learning models, and finalizes with results that unlock insight for clients and colleaguesDemonstrates skill in advanced statistical analysis, data mining, and/or research techniques, combined with broader awareness of the business and ongoing research, while functioning in a collaborative role with the Data Science team and across the wider organizationStays current with ongoing research in the field and brings new approaches to the teamServes as an internal expert resource and champion for data science within Marsh and MMCCoaches team members on the delivery of analytics-based tools and analyses and presentation of findings\n\nWe will count on you to\n\nMine and analyze data to drive optimization and implement business strategiesBuild machine learning algorithms based on the business askDeploy API on the platform by understanding the technical askDevelop statistical custom data models and algorithms from scratch to enhance current value proposition and new product developmentUtilize risk models related to Time Series Forecasting, Clustering, GLM/Regression, Boosting, Trees, etc.Use advanced analytics to augment consulting deliverables with data backed outputsConduct research on the clients risk areas and prepare a point of view for consulting\n\nQualifications\n\nMasters Degree in Math, Statistics, Data Science, Actuarial Science or related fieldExpert statistical modeling knowledge, including familiarity with machine learning techniquesAbility to face difficult and sometimes complex problemsAbility to develop strong internal/external client oriented solutionsSuperior detail orientation, excellent communication and interpersonal skillsKnowledge of modern programming languages such as Python, including NumPy, TensorFlow, SQL, and PyTorchR or VBA may be helpful but not required\n\nWhat makes you stand out?\n\nUnderstanding of insurance and risk managementKnowledge and demonstrated experience of advanced NLP and computer visionExperience of building data visualization in Plotly/Dash/D3.jsHands-on experience with stochastic or econometric modelling\n\nMarsh, a business of Marsh McLennan (NYSE: MMC), is the world’s top insurance broker and risk advisor. Marsh McLennan is a global leader in risk, strategy and people, advising clients in 130 countries across four businesses: Marsh, Guy Carpenter, Mercer and Oliver Wyman. With annual revenue of $24 billion and more than 90,000 colleagues, Marsh McLennan helps build the confidence to thrive through the power of perspective. For more information, visit marsh.com, or follow on LinkedIn and X.\n\nMarsh McLennan is committed to embracing a diverse, inclusive and flexible work environment. We aim to attract and retain the best people and embrace diversity of age, background, caste, disability, ethnic origin, family duties, gender orientation or expression, gender reassignment, marital status, nationality, parental status, personal or social status, political affiliation, race, religion and beliefs, sex/gender, sexual orientation or expression, skin color, or any other characteristic protected by applicable law.\n\nMarsh McLennan is committed to hybrid work, which includes the flexibility of working remotely and the collaboration, connections and professional development benefits of working together in the office. All Marsh McLennan colleagues are expected to be in their local office or working onsite with clients at least three days per week. Office-based teams will identify at least one “anchor day” per week on which their full team will be together in person.\n\nR_312699",
      "skills": [
        "Algorithms",
        "Artificial Intelligence (AI)",
        "Data Analytics",
        "Data Science",
        "Machine Learning",
        "Natural Language Processing (NLP)",
        "Pattern Recognition",
        "Python (Programming Language)",
        "SQL",
        "Analytics"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266333266",
      "title": "Senior MLOps Engineer",
      "company": "ProCogia",
      "location": "India",
      "workplace_type": "Remote",
      "experience_level": "",
      "job_url": "https://procogia.com/about-us/careers/?gh_jid=4770867008&gh_src=6d1e0c7e8us",
      "description": "About ProCogia:We’re a diverse, close-knit team with a common pursuit of providing top-class, end-to-end data solutions for our clients. In return for your talent and expertise, you will be rewarded with a competitive salary, generous benefits, alongwith ample opportunity for personal development. ‘Growth mindset’ is something we seek in all our new hires and has helped drive much of our recent growth across North America. Our distinct approach is to push the limits and value derived from data. Working within ProCogia’s thriving environment will allow you to unleash your full career potential.The core of our culture is maintaining a high level of cultural equality throughout the company. Our diversity and differences allow us to create innovative and effective data solutions for our clients.\nOur Core Values: Trust, Growth, Innovation, Excellence, and Ownership\nLocation: India (Remote)Time Zone: 12pm to 9pm IST\nJob Description:We are seeking a Senior MLOps Engineer with deep expertise in AWS CDK, MLOps, and Data Engineering tools to join a high-impact team focused on building reusable, scalable deployment pipelines for Amazon SageMaker workloads. This role combines hands-on engineering, automation, and infrastructure expertise with strong stakeholder engagement skills. You will work closely with Data Scientists, ML Engineers, and platform teams to accelerate ML productization using best-in-class DevOps practices.\nKey Responsibilities:Design, implement, and maintain reusable CI/CD pipelines for SageMaker-based ML workflows.Develop Infrastructure as Code using AWS CDK for scalable and secure cloud deployments.Build and manage integrations with AWS Lambda, Glue, Step Functions, and OpenTable formats (Apache Iceberg, Parquet, etc.).Support MLOps lifecycle: model packaging, deployment, versioning, monitoring, and rollback strategies.Use GitLab to manage repositories, pipelines, and infrastructure automation.Enable logging, monitoring, and cost-effective scaling of SageMaker instances and jobs.Collaborate closely with stakeholders across Data Science, Cloud Platform, and Product teams to gather requirements, communicate progress, and iterate on infrastructure designs.Ensure operational excellence through well-tested, reliable, and observable deployments.\nRequired Skills:2+ years of experience in MLOps, with 4+ years of experience in DevOps or Cloud Engineering, ideally with a focus on machine learning workloads.Hands-on experience with GitLab CI Pipelines, artifact scanning, vulnerability checks, and API management.Experience in Continuous Development, Continuous Integration (CI/CD), and Test-Driven Development (TDD).Experience in building microservices and API architectures using FastAPI, GraphQL, and Pydantic.Proficiency in Python v3.6 or higher and experience with Python frameworks such as Pytest.Strong experience with AWS CDK (TypeScript or Python) for IaC.Hands-on experience with Amazon SageMaker, including pipeline creation and model deployment.Solid command over AWS Lambda, AWS Glue, OpenTable formats (like Iceberg/Parquet), and event-driven architectures.Practical knowledge of MLOps best practices: reproducibility, metadata management, model drift, etc.Experience deploying production-grade data and ML systems.Comfortable working in a consulting/client-facing environment, with strong stakeholder management and communication skills\nPreferred Qualifications:Experience with feature stores, ML model registries, or custom SageMaker containers.Familiarity with data lineage, cost optimization, and cloud security best practices.Background in ML frameworks (TensorFlow, PyTorch, etc.).\nEducation: Bachelor’s or master’s degree in any of the following: statistics, data science, computer science, or another mathematically intensive field.\nProCogia is proud to be an equal-opportunity employer. We are committed to creating a diverse and inclusive workspace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.",
      "skills": [
        "Communication",
        "Data Science",
        "Engineering",
        "Mechanical Engineering",
        "Test Automation",
        "Cloud Development",
        "Computer-Aided Design (CAD)",
        "MLOps",
        "Stakeholder Engagement",
        "Stakeholder Management"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4264478280",
      "title": "SDET",
      "company": "Health Catalyst",
      "location": "Chandigarh, India",
      "workplace_type": "Remote",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/job-apply/4264478280",
      "description": "About us :The healthcare industry is the next great frontier of opportunity for software development, and Health Catalyst is one of the most dynamic and influential companies in this space. We are working on solving national-level healthcare problems, and this is your chance to improve the lives of millions of people, including your family and friends. Health Catalyst is a fast-growing company that values smart, hardworking, and humble individuals. Each product team is a small, mission-critical team focused on developing innovative tools to support Catalyst’s mission to improve healthcare performance, cost, and quality.Health Catalyst is expanding and maintains a large suite of Improvement Apps that contribute to healthcare analytics and process improvement solutions. This includes products that manage the care of health system populations, better serve patients at the point of care, reduce health system costs, and reduce clinician workload.\nJob Description:We’re seeking a skilled SDET with hands-on experience in Playwright automation using JavaScript/TypeScript and a solid understanding of performance testing. You’ll build and maintain automated test frameworks, collaborate with cross-functional teams, and help ensure high-quality, performant software releases.\nResponsibilities:Develop and maintain automated test scripts for web and API testing using Playwright (JS/TS).Build scalable, maintainable automation frameworks following best practices.Integrate automated tests into CI/CD pipelines (e.g., Azure DevOps).Collaborate with developers, QA, and product teams to define test strategies and plans.Troubleshoot automation issues and identify root causes.Conduct exploratory testing to complement automation coverage.Support performance testing initiatives using tools like JMeter or LoadRunner.Participate in Agile ceremonies and code reviews to uphold quality standards.Continuously improve testing processes and frameworks.Qualifications:3+ years of automation experience, primarily with Playwright in JavaScript/TypeScript.Experience with API testing tools (e.g., Postman).Familiarity with performance testing tools and methodologies is a strong plus.Solid understanding of various testing types: regression, system, acceptance, load, performance.Experience in Agile/Scrum environments.Proficient in SQL and database testing.Knowledge of CI/CD tools and version control (e.g., Azure DevOps, Git).Familiarity with containerization (Docker/Kubernetes) is desirable.Strong analytical, communication, and problem-solving skills.Self-motivated and eager to learn new technologies.",
      "skills": [
        "API Testing",
        "Continuous Integration and Continuous Delivery (CI/CD)",
        "JavaScript",
        "Programming",
        "SQL",
        "TypeScript",
        "Azure DevOps Services",
        "Playwriting"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266545269",
      "title": "Team Leader Sme ",
      "company": "Trafin Global Services Pvt. Ltd",
      "location": "New Delhi, Delhi, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/jobs/view/4266545269",
      "description": "Company Description\n \nWe suggest you enter details here.\n\n Role Description\n \nThis is a full-time on-site role for a Team Leader SME (Subject Matter Expert) at Trafin Global Services Pvt. Ltd. located in New Delhi. The Team Leader SME will be responsible for leading a team of subject matter experts, overseeing their work, providing guidance, and ensuring quality standards are met. Day-to-day tasks include developing strategies, coordinating with other departments, monitoring team performance, and reporting on progress. Additionally, the Team Leader SME will facilitate training sessions and provide expertise in addressing complex issues.\n\n Qualifications\n \n \\n Experience in team leadership, team coordination, and project management skillsSubject matter expertise in the relevant fieldExcellent problem-solving and decision-making abilitiesStrong communication and interpersonal skillsAbility to work on-site in New DelhiBachelor's degree in a related field or equivalent experienceExperience in the financial services or related industry is a plus",
      "skills": [
        "Analytical Skills",
        "Communication",
        "Interpersonal Skills",
        "Problem Solving",
        "Subject Matter Experts",
        "Team Coordination",
        "Team Leadership",
        "Team Management",
        "Team Performance",
        "Training"
      ],
      "search_role": "Machine Learning Engineer"
    },
    {
      "id": "4266542309",
      "title": "Java Software Engineer",
      "company": "Cozzera",
      "location": "Gurugram, Haryana, India",
      "workplace_type": "On-site",
      "experience_level": "",
      "job_url": "https://www.linkedin.com/job-apply/4266542309",
      "description": "Job DescriptionWe're looking for a Full Stack Developer with expertise in Java and React. As a Full Stack Developer, you'll be responsible for developing and maintaining web applications using Java, Spring Boot, and Hibernate for the backend, and React for the frontend.Technical SkillsJava: Strong knowledge of Java programming languageSpring Boot: Experience with Spring Boot frameworkHibernate: Knowledge of Hibernate ORM toolReact: Experience with React JavaScript libraryResponsibilitiesDevelop and maintain web applications using Java, Spring Boot, and HibernateDesign and implement frontend components using ReactCollaborate with cross-functional teams to deliver projectsTroubleshoot and resolve technical issuesBackground VerificationMandatory background verification checks include:Education CheckEmployment CheckDatabase CheckCriminal CheckAddress CheckCIBIL CheckReference CheckID CheckCV ValidationGap CheckCredit CheckDrug test",
      "skills": [
        "Java",
        "React.js"
      ],
      "search_role": "Machine Learning Engineer"
    }
  ]
}